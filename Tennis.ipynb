{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env_file_name = \"Tennis_Windows_x86_64/Tennis.exe\"\n",
    "# env = UnityEnvironment(file_name=env_file_name)\n",
    "env = UnityEnvironment(file_name=env_file_name,no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "states shape :  (2, 24)\n",
      "Both states look like :  [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -13.30557251  -3.          -0.           0.\n",
      "   13.66344166  12.          -0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -12.93397141  -3.           0.           0.\n",
      "  -13.66344166  12.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('states shape : ',states.shape)\n",
    "print('Both states look like : ',states)\n",
    "print(2*states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    total_scores = []\n",
    "    for i in range(100):                                        # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t = 0\n",
    "        while True:\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            # print('actions : ',actions)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            t += 1\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}, and {} steps taken'.format(i, np.max(scores),t))\n",
    "        print(scores)\n",
    "        total_scores.append(scores)\n",
    "    print('Average Random Score : ', np.mean(total_scores))\n",
    "        \n",
    "def plot_results(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    plt.ion()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.all_rewards)), results.all_rewards)\n",
    "    plt.plot(np.arange(len(results.avg_rewards)), results.avg_rewards)\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.critic_loss)), results.critic_loss)\n",
    "    plt.ylabel('critic_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.actor_loss)), results.actor_loss)\n",
    "    plt.ylabel('actor_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/2000   0% ETA:  --:--:-- |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.01\n",
      "action_size          : 2\n",
      "state_size           : 24\n",
      "hidden_size          : 256\n",
      "buffer_size          : 50000\n",
      "batch_size           : 512\n",
      "dropout              : 0.01\n",
      "seed                 : 89\n",
      "max_episodes         : 2000\n",
      "learn_every          : 10\n",
      "critic_learning_rate : 0.001\n",
      "actor_learning_rate  : 0.001\n",
      "noise_decay          : 0.9995\n",
      "num_agents           : 2\n",
      "env_file_name        : Tennis_Windows_x86_64/Tennis.exe\n",
      "train_mode           : True\n",
      "brain_name           : TennisBrain\n",
      "Running on device :  cpu\n",
      "Episode 0 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  1.000 || 0.068 seconds, mem : 15\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "episode: 20/2000   1% ETA:  0:01:42 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.990 || 0.045 seconds, mem : 299\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 40/2000   2% ETA:  0:01:29 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 40 \n",
      "update - q expected : mean : 0.0479 - sd : 0.0079 min-max 0.0221|0.0613\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0049 - sd : 0.0103 min-max -0.0499|0.0036\n",
      "Episode 40 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.980 || 0.243 seconds, mem : 583\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 50 \n",
      "update - q expected : mean : 0.0109 - sd : 0.0145 min-max -0.0483|0.0366\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0321 - sd : 0.0125 min-max -0.0150|0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 58/2000   2% ETA:  0:01:35 |\\                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 60 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0128 min-max -0.0109|0.0497\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0086 - sd : 0.0075 min-max -0.0312|0.0302\n",
      "Episode 60 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.970 || 0.105 seconds, mem : 867\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 70 \n",
      "update - q expected : mean : 0.0549 - sd : 0.0131 min-max 0.0188|0.0780\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0119 - sd : 0.0099 min-max -0.0588|0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 78/2000   3% ETA:  0:01:33 ||                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 80 \n",
      "update - q expected : mean : 0.0619 - sd : 0.0126 min-max 0.0326|0.0876\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0198 - sd : 0.0111 min-max -0.0655|-0.0040\n",
      "Episode 80 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.960 || 0.088 seconds, mem : 1151\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 90 \n",
      "update - q expected : mean : 0.0600 - sd : 0.0106 min-max 0.0331|0.0811\n",
      "update - reward : mean : -0.0002 - sd : 0.0012 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0170 - sd : 0.0100 min-max -0.0655|0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 98/2000   4% ETA:  0:01:32 |/                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 96 || Reward : [ 0.1  -0.01] || avg reward :  0.001 || Noise  0.953 || 0.068 seconds, mem : 1392\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 100 \n",
      "update - q expected : mean : 0.0553 - sd : 0.0089 min-max 0.0302|0.0707\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0120 - sd : 0.0101 min-max -0.0606|0.0081\n",
      "Episode 100 || Reward : [ 0.   -0.01] || avg reward :  0.001 || Noise  0.951 || 0.088 seconds, mem : 1449\n",
      "\u001b[0m\u001b[41mEpisode 104 || Reward : [0.09 0.1 ] || avg reward :  0.002 || Noise  0.949 || 0.120 seconds, mem : 1558\n",
      "\u001b[0m\u001b[41mEpisode 109 || Reward : [ 0.1  -0.01] || avg reward :  0.003 || Noise  0.946 || 0.071 seconds, mem : 1637\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 110 \n",
      "update - q expected : mean : 0.0500 - sd : 0.0082 min-max 0.0186|0.0620\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0061 - sd : 0.0091 min-max -0.0540|0.0163\n",
      "\u001b[41mEpisode 110 || Reward : [-0.01  0.1 ] || avg reward :  0.004 || Noise  0.946 || 0.143 seconds, mem : 1667\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 115/2000   5% ETA:  0:01:35 |--                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 114 || Reward : [-0.01  0.1 ] || avg reward :  0.005 || Noise  0.944 || 0.064 seconds, mem : 1732\n",
      "\u001b[0m\u001b[41mEpisode 117 || Reward : [ 0.1  -0.01] || avg reward :  0.006 || Noise  0.943 || 0.061 seconds, mem : 1791\n",
      "\u001b[0m\u001b[41mEpisode 119 || Reward : [-0.01  0.1 ] || avg reward :  0.007 || Noise  0.942 || 0.068 seconds, mem : 1827\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 120 \n",
      "update - q expected : mean : 0.0452 - sd : 0.0086 min-max 0.0162|0.0556\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0021 - sd : 0.0108 min-max -0.0528|0.1031\n",
      "Episode 120 || Reward : [-0.01  0.  ] || avg reward :  0.007 || Noise  0.941 || 0.088 seconds, mem : 1841\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 135/2000   6% ETA:  0:01:34 |\\\\                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 130 \n",
      "update - q expected : mean : 0.0411 - sd : 0.0098 min-max 0.0042|0.0530\n",
      "\u001b[42mupdate - reward : mean : 0.0004 - sd : 0.0078 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0038 - sd : 0.0113 min-max -0.0457|0.1069\n",
      "\u001b[41mEpisode 130 || Reward : [ 0.1  -0.01] || avg reward :  0.008 || Noise  0.937 || 0.103 seconds, mem : 1988\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 140 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0110 min-max -0.0008|0.0510\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0200|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0063 - sd : 0.0098 min-max -0.0582|0.1082\n",
      "Episode 140 || Reward : [-0.02  0.  ] || avg reward :  0.008 || Noise  0.932 || 0.110 seconds, mem : 2135\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 155/2000   7% ETA:  0:01:31 ||||                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 150 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0117 min-max -0.0075|0.0506\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : 0.0066 - sd : 0.0100 min-max -0.0561|0.0490\n",
      "--------------------------------------\n",
      "Agent 0 and episode 160 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0123 min-max -0.0123|0.0510\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0068 - sd : 0.0109 min-max -0.0369|0.1094\n",
      "Episode 160 || Reward : [ 0.   -0.01] || avg reward :  0.008 || Noise  0.923 || 0.088 seconds, mem : 2419\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 170 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0120 min-max -0.0099|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0062 - sd : 0.0092 min-max -0.0373|0.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 175/2000   8% ETA:  0:01:30 |///                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 180 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0113 min-max -0.0079|0.0518\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0054 - sd : 0.0093 min-max -0.0379|0.1052\n",
      "Episode 180 || Reward : [-0.01  0.  ] || avg reward :  0.008 || Noise  0.913 || 0.097 seconds, mem : 2703\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 190 \n",
      "update - q expected : mean : 0.0401 - sd : 0.0105 min-max -0.0008|0.0517\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0024 - sd : 0.0094 min-max -0.0383|0.0261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 195/2000   9% ETA:  0:01:28 |---                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 200 \n",
      "update - q expected : mean : 0.0416 - sd : 0.0104 min-max -0.0008|0.0517\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0017 - sd : 0.0101 min-max -0.0378|0.1068\n",
      "Episode 200 || Reward : [-0.01  0.  ] || avg reward :  0.007 || Noise  0.904 || 0.111 seconds, mem : 2987\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 210 \n",
      "update - q expected : mean : 0.0430 - sd : 0.0096 min-max 0.0061|0.0530\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0094 min-max -0.0397|0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 215/2000  10% ETA:  0:01:27 |\\\\\\\\                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 220 \n",
      "update - q expected : mean : 0.0440 - sd : 0.0095 min-max 0.0023|0.0538\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0086 min-max -0.0362|0.0255\n",
      "Episode 220 || Reward : [ 0.   -0.01] || avg reward :  0.001 || Noise  0.895 || 0.116 seconds, mem : 3271\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 230 \n",
      "update - q expected : mean : 0.0444 - sd : 0.0098 min-max 0.0042|0.0556\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0012 - sd : 0.0094 min-max -0.0339|0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 235/2000  11% ETA:  0:01:26 |||||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 240 \n",
      "update - q expected : mean : 0.0449 - sd : 0.0103 min-max 0.0052|0.0558\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0026 - sd : 0.0080 min-max -0.0513|0.0227\n",
      "Episode 240 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.886 || 0.086 seconds, mem : 3555\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 250 \n",
      "update - q expected : mean : 0.0447 - sd : 0.0109 min-max 0.0069|0.0561\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0017 - sd : 0.0091 min-max -0.0310|0.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 255/2000  12% ETA:  0:01:24 |////                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 260 \n",
      "update - q expected : mean : 0.0438 - sd : 0.0123 min-max 0.0016|0.0559\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0085 min-max -0.0293|0.0556\n",
      "Episode 260 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.878 || 0.093 seconds, mem : 3839\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 270 \n",
      "update - q expected : mean : 0.0437 - sd : 0.0126 min-max -0.0053|0.0555\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0082 min-max -0.0334|0.0287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 275/2000  13% ETA:  0:01:23 |-----                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 280 \n",
      "update - q expected : mean : 0.0435 - sd : 0.0122 min-max 0.0053|0.0549\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0015 - sd : 0.0083 min-max -0.0312|0.0537\n",
      "Episode 280 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.869 || 0.086 seconds, mem : 4123\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 290 \n",
      "update - q expected : mean : 0.0436 - sd : 0.0128 min-max -0.0071|0.0547\n",
      "update - reward : mean : -0.0002 - sd : 0.0016 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0073 min-max -0.0465|0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 295/2000  14% ETA:  0:01:22 |\\\\\\\\\\                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 300 \n",
      "update - q expected : mean : 0.0434 - sd : 0.0129 min-max -0.0014|0.0544\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0010 - sd : 0.0074 min-max -0.0259|0.0550\n",
      "Episode 300 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.860 || 0.089 seconds, mem : 4407\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 310 \n",
      "update - q expected : mean : 0.0433 - sd : 0.0126 min-max -0.0011|0.0546\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0089 min-max -0.0286|0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 315/2000  15% ETA:  0:01:21 |||||||                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 320 \n",
      "update - q expected : mean : 0.0435 - sd : 0.0125 min-max 0.0034|0.0547\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0015 - sd : 0.0078 min-max -0.0468|0.0183\n",
      "Episode 320 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.852 || 0.088 seconds, mem : 4691\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 330 \n",
      "update - q expected : mean : 0.0436 - sd : 0.0124 min-max -0.0019|0.0545\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0025 - sd : 0.0073 min-max -0.0352|0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 335/2000  16% ETA:  0:01:20 |//////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 340 \n",
      "update - q expected : mean : 0.0436 - sd : 0.0123 min-max -0.0013|0.0542\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0013 - sd : 0.0066 min-max -0.0353|0.0200\n",
      "Episode 340 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.843 || 0.123 seconds, mem : 4975\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 353/2000  17% ETA:  0:01:19 |------                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 350 \n",
      "update - q expected : mean : 0.0427 - sd : 0.0130 min-max -0.0083|0.0539\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0066 min-max -0.0339|0.0206\n",
      "--------------------------------------\n",
      "Agent 0 and episode 360 \n",
      "update - q expected : mean : 0.0424 - sd : 0.0132 min-max -0.0078|0.0539\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0071 min-max -0.0317|0.0280\n",
      "Episode 360 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.835 || 0.106 seconds, mem : 5259\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 371/2000  18% ETA:  0:01:19 |\\\\\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 370 \n",
      "update - q expected : mean : 0.0425 - sd : 0.0127 min-max -0.0025|0.0538\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0079 min-max -0.0433|0.1017\n",
      "--------------------------------------\n",
      "Agent 0 and episode 380 \n",
      "update - q expected : mean : 0.0417 - sd : 0.0134 min-max -0.0019|0.0535\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0018 - sd : 0.0072 min-max -0.0305|0.0204\n",
      "Episode 380 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.827 || 0.103 seconds, mem : 5543\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 391/2000  19% ETA:  0:01:18 ||||||||                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 390 \n",
      "update - q expected : mean : 0.0416 - sd : 0.0130 min-max -0.0018|0.0532\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0009 - sd : 0.0064 min-max -0.0245|0.0207\n",
      "--------------------------------------\n",
      "Agent 0 and episode 400 \n",
      "update - q expected : mean : 0.0410 - sd : 0.0140 min-max -0.0076|0.0531\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0057 min-max -0.0202|0.0270\n",
      "Episode 400 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.818 || 0.086 seconds, mem : 5827\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 411/2000  20% ETA:  0:01:17 |////////                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 410 \n",
      "update - q expected : mean : 0.0405 - sd : 0.0141 min-max -0.0069|0.0528\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0081 min-max -0.0465|0.1006\n",
      "--------------------------------------\n",
      "Agent 0 and episode 420 \n",
      "update - q expected : mean : 0.0407 - sd : 0.0130 min-max -0.0070|0.0526\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0070 min-max -0.0463|0.0379\n",
      "Episode 420 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.810 || 0.095 seconds, mem : 6111\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 431/2000  21% ETA:  0:01:16 |--------                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 430 \n",
      "update - q expected : mean : 0.0406 - sd : 0.0137 min-max -0.0089|0.0523\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0089 min-max -0.0458|0.1053\n",
      "--------------------------------------\n",
      "Agent 0 and episode 440 \n",
      "update - q expected : mean : 0.0404 - sd : 0.0132 min-max -0.0068|0.0524\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0077 min-max -0.0452|0.1073\n",
      "Episode 440 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.802 || 0.095 seconds, mem : 6395\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 451/2000  22% ETA:  0:01:14 |\\\\\\\\\\\\\\\\                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 450 \n",
      "update - q expected : mean : 0.0404 - sd : 0.0123 min-max -0.0008|0.0521\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0061 min-max -0.0261|0.0217\n",
      "--------------------------------------\n",
      "Agent 0 and episode 460 \n",
      "update - q expected : mean : 0.0409 - sd : 0.0118 min-max -0.0042|0.0520\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0061 min-max -0.0450|0.0139\n",
      "Episode 460 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.794 || 0.097 seconds, mem : 6679\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 471/2000  23% ETA:  0:01:13 ||||||||||                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 470 \n",
      "update - q expected : mean : 0.0407 - sd : 0.0115 min-max -0.0052|0.0519\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0057 min-max -0.0244|0.0233\n",
      "--------------------------------------\n",
      "Agent 0 and episode 480 \n",
      "update - q expected : mean : 0.0405 - sd : 0.0115 min-max -0.0022|0.0519\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0062 min-max -0.0217|0.0592\n",
      "Episode 480 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.786 || 0.089 seconds, mem : 6963\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 491/2000  24% ETA:  0:01:12 |/////////                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 490 \n",
      "update - q expected : mean : 0.0403 - sd : 0.0116 min-max -0.0019|0.0519\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0083 min-max -0.0359|0.1003\n",
      "--------------------------------------\n",
      "Agent 0 and episode 500 \n",
      "update - q expected : mean : 0.0406 - sd : 0.0114 min-max -0.0042|0.0517\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0060 min-max -0.0240|0.0259\n",
      "Episode 500 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.778 || 0.097 seconds, mem : 7247\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 511/2000  25% ETA:  0:01:11 |---------                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 507 || Reward : [0.1  0.09] || avg reward :  0.001 || Noise  0.776 || 0.132 seconds, mem : 7384\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 510 \n",
      "update - q expected : mean : 0.0407 - sd : 0.0109 min-max -0.0021|0.0516\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0083 min-max -0.0416|0.1031\n",
      "\u001b[41mEpisode 514 || Reward : [ 0.1  -0.01] || avg reward :  0.002 || Noise  0.773 || 0.071 seconds, mem : 7499\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 520 \n",
      "update - q expected : mean : 0.0403 - sd : 0.0115 min-max -0.0088|0.0518\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0013 - sd : 0.0089 min-max -0.0435|0.1001\n",
      "Episode 520 || Reward : [-0.01  0.  ] || avg reward :  0.002 || Noise  0.771 || 0.104 seconds, mem : 7583\n",
      "\u001b[0m\u001b[41mEpisode 524 || Reward : [ 0.1  -0.01] || avg reward :  0.003 || Noise  0.769 || 0.094 seconds, mem : 7656\n",
      "\u001b[0m\u001b[41mEpisode 526 || Reward : [-0.01  0.1 ] || avg reward :  0.004 || Noise  0.768 || 0.127 seconds, mem : 7732\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 529/2000  26% ETA:  0:01:11 |\\\\\\\\\\\\\\\\\\\\                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 527 || Reward : [ 0.1  -0.01] || avg reward :  0.005 || Noise  0.768 || 0.087 seconds, mem : 7763\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 530 \n",
      "update - q expected : mean : 0.0400 - sd : 0.0122 min-max -0.0087|0.0516\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0075 min-max -0.0366|0.1086\n",
      "\u001b[41mEpisode 534 || Reward : [ 0.1  -0.01] || avg reward :  0.006 || Noise  0.765 || 0.082 seconds, mem : 7883\n",
      "\u001b[0m\u001b[41mEpisode 538 || Reward : [0.   0.09] || avg reward :  0.007 || Noise  0.764 || 0.092 seconds, mem : 7956\n",
      "\u001b[0m\u001b[41mEpisode 539 || Reward : [-0.01  0.1 ] || avg reward :  0.008 || Noise  0.763 || 0.113 seconds, mem : 7987\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 540 \n",
      "update - q expected : mean : 0.0393 - sd : 0.0130 min-max -0.0133|0.0516\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0010 - sd : 0.0064 min-max -0.0330|0.0299\n",
      "Episode 540 || Reward : [-0.01  0.  ] || avg reward :  0.008 || Noise  0.763 || 0.106 seconds, mem : 8002\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 545/2000  27% ETA:  0:01:10 |||||||||||                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 543 || Reward : [ 0.1  -0.01] || avg reward :  0.009 || Noise  0.762 || 0.165 seconds, mem : 8094\n",
      "\u001b[0m\u001b[41mEpisode 545 || Reward : [-0.01  0.1 ] || avg reward :  0.010 || Noise  0.761 || 0.163 seconds, mem : 8159\n",
      "\u001b[0m\u001b[41mEpisode 549 || Reward : [ 0.1  -0.01] || avg reward :  0.011 || Noise  0.760 || 0.069 seconds, mem : 8231\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 550 \n",
      "update - q expected : mean : 0.0387 - sd : 0.0140 min-max -0.0124|0.0520\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0069 min-max -0.0436|0.0429\n",
      "\u001b[41mEpisode 550 || Reward : [ 0.1  -0.01] || avg reward :  0.012 || Noise  0.759 || 0.196 seconds, mem : 8279\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 561/2000  28% ETA:  0:01:11 |//////////                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 560 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0150 min-max -0.0209|0.0519\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0017 - sd : 0.0068 min-max -0.0424|0.0351\n",
      "Episode 560 || Reward : [ 0.   -0.01] || avg reward :  0.012 || Noise  0.755 || 0.103 seconds, mem : 8458\n",
      "\u001b[0m\u001b[41mEpisode 561 || Reward : [0.1  0.09] || avg reward :  0.013 || Noise  0.755 || 0.130 seconds, mem : 8510\n",
      "\u001b[0m\u001b[41mEpisode 567 || Reward : [ 0.1  -0.01] || avg reward :  0.014 || Noise  0.753 || 0.105 seconds, mem : 8611\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 570 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0133 min-max -0.0133|0.0529\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0076 min-max -0.0361|0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 578/2000  28% ETA:  0:01:10 |-----------                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 577 || Reward : [0.1  0.09] || avg reward :  0.015 || Noise  0.749 || 0.133 seconds, mem : 8804\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 580 \n",
      "update - q expected : mean : 0.0393 - sd : 0.0129 min-max -0.0120|0.0526\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0077 min-max -0.0499|0.0274\n",
      "Episode 580 || Reward : [ 0.   -0.01] || avg reward :  0.015 || Noise  0.748 || 0.100 seconds, mem : 8846\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 590 \n",
      "update - q expected : mean : 0.0398 - sd : 0.0131 min-max -0.0131|0.0531\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0057 min-max -0.0233|0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 598/2000  29% ETA:  0:01:09 |\\\\\\\\\\\\\\\\\\\\\\                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 600 \n",
      "update - q expected : mean : 0.0402 - sd : 0.0119 min-max -0.0071|0.0534\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0083 min-max -0.0491|0.0893\n",
      "Episode 600 || Reward : [-0.01  0.  ] || avg reward :  0.015 || Noise  0.740 || 0.092 seconds, mem : 9130\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 610 \n",
      "update - q expected : mean : 0.0400 - sd : 0.0123 min-max -0.0072|0.0532\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0016 - sd : 0.0068 min-max -0.0517|0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 618/2000  30% ETA:  0:01:08 |||||||||||||                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 620 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0132 min-max -0.0126|0.0528\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0079 min-max -0.0600|0.0999\n",
      "Episode 620 || Reward : [ 0.   -0.01] || avg reward :  0.013 || Noise  0.733 || 0.091 seconds, mem : 9414\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 630 \n",
      "update - q expected : mean : 0.0393 - sd : 0.0134 min-max -0.0157|0.0531\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0075 min-max -0.0457|0.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 638/2000  31% ETA:  0:01:06 |////////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 640 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0142 min-max -0.0173|0.0513\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0077 min-max -0.0595|0.1015\n",
      "Episode 640 || Reward : [-0.01  0.  ] || avg reward :  0.007 || Noise  0.726 || 0.097 seconds, mem : 9698\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 650 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0130 min-max -0.0126|0.0516\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0056 min-max -0.0286|0.0241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 658/2000  32% ETA:  0:01:05 |------------                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 660 \n",
      "update - q expected : mean : 0.0393 - sd : 0.0127 min-max -0.0144|0.0514\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0063 min-max -0.0469|0.0262\n",
      "Episode 660 || Reward : [ 0.   -0.01] || avg reward :  0.003 || Noise  0.719 || 0.097 seconds, mem : 9982\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 670 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0131 min-max -0.0124|0.0518\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0072 min-max -0.0272|0.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 678/2000  33% ETA:  0:01:04 |\\\\\\\\\\\\\\\\\\\\\\\\\\                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 680 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0126 min-max -0.0242|0.0516\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0073 min-max -0.0282|0.0992\n",
      "Episode 680 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.711 || 0.120 seconds, mem : 10266\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 690 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0122 min-max -0.0169|0.0509\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0082 min-max -0.0371|0.1094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 698/2000  34% ETA:  0:01:03 ||||||||||||||                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 700 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0116 min-max -0.0077|0.0530\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0066 min-max -0.0331|0.0995\n",
      "Episode 700 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.704 || 0.088 seconds, mem : 10550\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 710 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0121 min-max -0.0205|0.0514\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0053 min-max -0.0586|0.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 718/2000  35% ETA:  0:01:02 |//////////////                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 720 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0120 min-max -0.0207|0.0530\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0055 min-max -0.0311|0.0217\n",
      "Episode 720 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.697 || 0.101 seconds, mem : 10834\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 730 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0136 min-max -0.0233|0.0518\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0079 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0089 min-max -0.0382|0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 738/2000  36% ETA:  0:01:01 |--------------                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 740 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0135 min-max -0.0203|0.0509\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0052 min-max -0.0369|0.0222\n",
      "Episode 740 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.690 || 0.096 seconds, mem : 11118\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 750 \n",
      "update - q expected : mean : 0.0382 - sd : 0.0123 min-max -0.0230|0.0509\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0014 - sd : 0.0063 min-max -0.0397|0.0312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 758/2000  37% ETA:  0:01:00 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 760 \n",
      "update - q expected : mean : 0.0382 - sd : 0.0130 min-max -0.0162|0.0527\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0055 min-max -0.0342|0.0220\n",
      "Episode 760 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.683 || 0.085 seconds, mem : 11402\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 770 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0126 min-max -0.0159|0.0510\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0057 min-max -0.0342|0.0208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 778/2000  38% ETA:  0:00:59 ||||||||||||||||                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 780 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0121 min-max -0.0101|0.0510\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0066 min-max -0.0246|0.0965\n",
      "Episode 780 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.677 || 0.106 seconds, mem : 11686\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 790 \n",
      "update - q expected : mean : 0.0380 - sd : 0.0135 min-max -0.0154|0.0520\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0065 min-max -0.0282|0.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 798/2000  39% ETA:  0:00:58 |///////////////                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 800 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0134 min-max -0.0188|0.0529\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0067 min-max -0.0238|0.1047\n",
      "Episode 800 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.670 || 0.095 seconds, mem : 11970\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 810 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0127 min-max -0.0205|0.0513\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0045 min-max -0.0178|0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 818/2000  40% ETA:  0:00:57 |---------------                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 820 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0127 min-max -0.0120|0.0509\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0058 min-max -0.0566|0.0182\n",
      "Episode 820 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.663 || 0.108 seconds, mem : 12254\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 830 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0131 min-max -0.0173|0.0508\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0078 min-max -0.0255|0.1039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 838/2000  41% ETA:  0:00:55 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 840 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0133 min-max -0.0168|0.0528\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0051 min-max -0.0285|0.0239\n",
      "Episode 840 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.657 || 0.087 seconds, mem : 12538\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 850 \n",
      "update - q expected : mean : 0.0387 - sd : 0.0123 min-max -0.0202|0.0546\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0068 min-max -0.0379|0.0970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 858/2000  42% ETA:  0:00:54 |||||||||||||||||                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 860 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0138 min-max -0.0270|0.0507\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0068 min-max -0.0340|0.1022\n",
      "Episode 860 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.650 || 0.090 seconds, mem : 12822\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 870 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0132 min-max -0.0233|0.0506\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0011 - sd : 0.0051 min-max -0.0418|0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 878/2000  43% ETA:  0:00:53 |/////////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 880 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0126 min-max -0.0186|0.0507\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0052 min-max -0.0300|0.0214\n",
      "Episode 880 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.644 || 0.091 seconds, mem : 13106\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 890 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0131 min-max -0.0200|0.0510\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0054 min-max -0.0364|0.0218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 898/2000  44% ETA:  0:00:52 |-----------------                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 900 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0123 min-max -0.0092|0.0513\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0047 min-max -0.0341|0.0173\n",
      "Episode 900 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.637 || 0.098 seconds, mem : 13390\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 910 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0132 min-max -0.0214|0.0503\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0047 min-max -0.0186|0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 918/2000  45% ETA:  0:00:51 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 920 \n",
      "update - q expected : mean : 0.0380 - sd : 0.0116 min-max -0.0189|0.0506\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0078 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0011 - sd : 0.0086 min-max -0.0198|0.1013\n",
      "Episode 920 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.631 || 0.102 seconds, mem : 13674\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 930 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0146 min-max -0.0235|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0041 min-max -0.0157|0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 938/2000  46% ETA:  0:00:50 |||||||||||||||||||                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 940 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0133 min-max -0.0334|0.0504\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0018 - sd : 0.0076 min-max -0.0400|0.1026\n",
      "Episode 940 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.625 || 0.085 seconds, mem : 13958\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 950 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0129 min-max -0.0174|0.0510\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0047 min-max -0.0333|0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 958/2000  47% ETA:  0:00:49 |//////////////////                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 960 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0134 min-max -0.0164|0.0509\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0079 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0092 min-max -0.0380|0.1022\n",
      "Episode 960 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.618 || 0.093 seconds, mem : 14242\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 970 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0127 min-max -0.0246|0.0519\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0061 min-max -0.0275|0.0954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 978/2000  48% ETA:  0:00:48 |-------------------                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 980 \n",
      "update - q expected : mean : 0.0387 - sd : 0.0130 min-max -0.0145|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0017 - sd : 0.0044 min-max -0.0270|0.0108\n",
      "Episode 980 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.612 || 0.110 seconds, mem : 14526\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 990 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0134 min-max -0.0204|0.0503\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0062 min-max -0.0304|0.1018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 997/2000  49% ETA:  0:00:48 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1000 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0141 min-max -0.0212|0.0503\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0041 min-max -0.0177|0.0180\n",
      "Episode 1000 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.606 || 0.107 seconds, mem : 14810\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1010 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0143 min-max -0.0248|0.0508\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0043 min-max -0.0358|0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1017/2000  50% ETA:  0:00:47 ||||||||||||||||||||                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1020 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0141 min-max -0.0199|0.0503\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0047 min-max -0.0549|0.0143\n",
      "Episode 1020 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.600 || 0.104 seconds, mem : 15094\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1030 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0137 min-max -0.0175|0.0507\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0043 min-max -0.0213|0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1037/2000  51% ETA:  0:00:46 |///////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1040 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0135 min-max -0.0209|0.0525\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0008 - sd : 0.0060 min-max -0.0175|0.1003\n",
      "Episode 1040 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.594 || 0.091 seconds, mem : 15378\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1050 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0137 min-max -0.0274|0.0504\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0013 - sd : 0.0053 min-max -0.0204|0.0829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1057/2000  52% ETA:  0:00:45 |--------------------                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1060 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0124 min-max -0.0155|0.0506\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0062 min-max -0.0347|0.0946\n",
      "Episode 1060 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.588 || 0.120 seconds, mem : 15662\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1070 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0132 min-max -0.0181|0.0507\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0034 min-max -0.0160|0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1077/2000  53% ETA:  0:00:44 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1080 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0135 min-max -0.0163|0.0503\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0057 min-max -0.0310|0.0835\n",
      "Episode 1080 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.582 || 0.090 seconds, mem : 15946\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1090 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0149 min-max -0.0235|0.0558\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0035 min-max -0.0136|0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1097/2000  54% ETA:  0:00:43 |||||||||||||||||||||                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1100 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0136 min-max -0.0185|0.0508\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0036 min-max -0.0157|0.0154\n",
      "Episode 1100 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.577 || 0.093 seconds, mem : 16230\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1110 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0144 min-max -0.0190|0.0547\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0053 min-max -0.0248|0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1117/2000  55% ETA:  0:00:42 |/////////////////////                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1120 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0135 min-max -0.0199|0.0582\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0054 min-max -0.0360|0.0831\n",
      "Episode 1120 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.571 || 0.094 seconds, mem : 16514\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1130 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0126 min-max -0.0200|0.0512\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0035 min-max -0.0173|0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1137/2000  56% ETA:  0:00:41 |---------------------                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1140 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0138 min-max -0.0231|0.0507\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0034 min-max -0.0180|0.0131\n",
      "Episode 1140 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.565 || 0.101 seconds, mem : 16798\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1150 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0125 min-max -0.0156|0.0508\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0038 min-max -0.0177|0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1157/2000  57% ETA:  0:00:40 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1160 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0143 min-max -0.0196|0.0507\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0061 min-max -0.0342|0.0999\n",
      "Episode 1160 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.560 || 0.090 seconds, mem : 17082\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1170 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0136 min-max -0.0237|0.0546\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0035 min-max -0.0161|0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1177/2000  58% ETA:  0:00:39 |||||||||||||||||||||||                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1180 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0146 min-max -0.0200|0.0547\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0059 min-max -0.0327|0.1016\n",
      "Episode 1180 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.554 || 0.118 seconds, mem : 17366\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1190 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0132 min-max -0.0152|0.0506\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0038 min-max -0.0198|0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1197/2000  59% ETA:  0:00:38 |//////////////////////                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1200 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0138 min-max -0.0173|0.0542\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0038 min-max -0.0299|0.0087\n",
      "Episode 1200 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.548 || 0.114 seconds, mem : 17650\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1210 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0146 min-max -0.0190|0.0506\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0041 min-max -0.0376|0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1217/2000  60% ETA:  0:00:37 |-----------------------               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1220 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0145 min-max -0.0216|0.0502\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0008 - sd : 0.0035 min-max -0.0245|0.0149\n",
      "Episode 1220 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.543 || 0.100 seconds, mem : 17934\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1230 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0143 min-max -0.0225|0.0509\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0039 min-max -0.0238|0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1237/2000  61% ETA:  0:00:36 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1240 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0138 min-max -0.0136|0.0506\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0038 min-max -0.0222|0.0106\n",
      "Episode 1240 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.538 || 0.103 seconds, mem : 18218\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1250 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0137 min-max -0.0166|0.0504\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0057 min-max -0.0305|0.0995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1257/2000  62% ETA:  0:00:35 ||||||||||||||||||||||||               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1260 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0135 min-max -0.0155|0.0505\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0048 min-max -0.0156|0.0848\n",
      "Episode 1260 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.532 || 0.099 seconds, mem : 18502\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1270 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0148 min-max -0.0211|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0032 min-max -0.0206|0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1277/2000  63% ETA:  0:00:34 |////////////////////////              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1280 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0141 min-max -0.0176|0.0528\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0010 - sd : 0.0050 min-max -0.0162|0.0800\n",
      "Episode 1280 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.527 || 0.091 seconds, mem : 18786\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1290 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0144 min-max -0.0276|0.0515\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0036 min-max -0.0183|0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1297/2000  64% ETA:  0:00:33 |------------------------              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1300 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0140 min-max -0.0225|0.0515\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0038 min-max -0.0309|0.0125\n",
      "Episode 1300 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.522 || 0.103 seconds, mem : 19070\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1310 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0135 min-max -0.0247|0.0496\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0008 - sd : 0.0056 min-max -0.0194|0.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1317/2000  65% ETA:  0:00:32 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1320 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0137 min-max -0.0156|0.0512\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0031 min-max -0.0146|0.0114\n",
      "Episode 1320 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.517 || 0.095 seconds, mem : 19354\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1330 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0135 min-max -0.0110|0.0502\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0057 min-max -0.0308|0.0986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1337/2000  66% ETA:  0:00:31 ||||||||||||||||||||||||||             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1340 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0142 min-max -0.0153|0.0520\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0057 min-max -0.0217|0.1016\n",
      "Episode 1340 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.511 || 0.090 seconds, mem : 19638\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1350 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0159 min-max -0.0177|0.0498\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0033 min-max -0.0132|0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1357/2000  67% ETA:  0:00:30 |/////////////////////////             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1360 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0157 min-max -0.0218|0.0545\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0011 - sd : 0.0070 min-max -0.0202|0.1037\n",
      "Episode 1360 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.506 || 0.086 seconds, mem : 19922\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1370 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0154 min-max -0.0194|0.0586\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0050 min-max -0.0145|0.0886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1377/2000  68% ETA:  0:00:29 |--------------------------            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1380 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0142 min-max -0.0187|0.0510\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0053 min-max -0.0217|0.0968\n",
      "Episode 1380 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.501 || 0.083 seconds, mem : 20206\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1390 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0138 min-max -0.0147|0.0505\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0035 min-max -0.0186|0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1397/2000  69% ETA:  0:00:28 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1400 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0133 min-max -0.0169|0.0533\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0033 min-max -0.0289|0.0116\n",
      "Episode 1400 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.496 || 0.094 seconds, mem : 20490\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1410 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0130 min-max -0.0117|0.0643\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0052 min-max -0.0286|0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1417/2000  70% ETA:  0:00:27 |||||||||||||||||||||||||||            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1420 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0143 min-max -0.0188|0.0505\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0033 min-max -0.0224|0.0148\n",
      "Episode 1420 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.491 || 0.094 seconds, mem : 20774\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1430 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0148 min-max -0.0138|0.0544\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0008 - sd : 0.0053 min-max -0.0293|0.0855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1437/2000  71% ETA:  0:00:26 |///////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1440 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0160 min-max -0.0235|0.0502\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0006 - sd : 0.0031 min-max -0.0131|0.0145\n",
      "Episode 1440 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.486 || 0.094 seconds, mem : 21058\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1450 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0165 min-max -0.0296|0.0521\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0017 - sd : 0.0038 min-max -0.0170|0.0223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1457/2000  72% ETA:  0:00:25 |---------------------------           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1460 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0143 min-max -0.0135|0.0507\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0009 - sd : 0.0033 min-max -0.0284|0.0079\n",
      "Episode 1460 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.482 || 0.098 seconds, mem : 21342\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1470 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0132 min-max -0.0123|0.0530\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0059 min-max -0.0291|0.1026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1477/2000  73% ETA:  0:00:24 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1480 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0136 min-max -0.0161|0.0615\n",
      "update - reward : mean : -0.0002 - sd : 0.0012 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0030 min-max -0.0234|0.0092\n",
      "Episode 1480 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.477 || 0.099 seconds, mem : 21626\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1490 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0138 min-max -0.0143|0.0774\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0062 min-max -0.0194|0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1497/2000  74% ETA:  0:00:23 |||||||||||||||||||||||||||||          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1500 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0141 min-max -0.0170|0.0531\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0031 min-max -0.0286|0.0094\n",
      "Episode 1500 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.472 || 0.120 seconds, mem : 21910\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1510 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0157 min-max -0.0202|0.0527\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0032 min-max -0.0175|0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1517/2000  75% ETA:  0:00:22 |////////////////////////////          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1520 \n",
      "update - q expected : mean : 0.0352 - sd : 0.0161 min-max -0.0222|0.0508\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0008 - sd : 0.0033 min-max -0.0204|0.0146\n",
      "Episode 1520 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.467 || 0.085 seconds, mem : 22194\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1530 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0144 min-max -0.0192|0.0501\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0031 min-max -0.0318|0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1537/2000  76% ETA:  0:00:21 |-----------------------------         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1540 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0141 min-max -0.0123|0.0511\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0033 min-max -0.0186|0.0107\n",
      "Episode 1540 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.463 || 0.109 seconds, mem : 22478\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1550 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0140 min-max -0.0122|0.0499\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0028 min-max -0.0128|0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1557/2000  77% ETA:  0:00:20 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1560 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0152 min-max -0.0165|0.0498\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0028 min-max -0.0142|0.0092\n",
      "Episode 1560 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.458 || 0.083 seconds, mem : 22762\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1570 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0163 min-max -0.0266|0.0793\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0011 - sd : 0.0037 min-max -0.0278|0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1577/2000  78% ETA:  0:00:19 ||||||||||||||||||||||||||||||         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1580 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0141 min-max -0.0135|0.0786\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0044 min-max -0.0202|0.0688\n",
      "Episode 1580 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.454 || 0.095 seconds, mem : 23046\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1590 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0149 min-max -0.0161|0.0670\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0008 - sd : 0.0043 min-max -0.0223|0.0611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1597/2000  79% ETA:  0:00:19 |//////////////////////////////        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1600 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0146 min-max -0.0155|0.0506\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0028 min-max -0.0185|0.0118\n",
      "Episode 1600 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.449 || 0.119 seconds, mem : 23330\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1610 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0158 min-max -0.0169|0.0594\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0037 min-max -0.0337|0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1617/2000  80% ETA:  0:00:18 |------------------------------        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1620 \n",
      "update - q expected : mean : 0.0350 - sd : 0.0164 min-max -0.0229|0.0527\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0056 min-max -0.0191|0.0998\n",
      "Episode 1620 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.445 || 0.116 seconds, mem : 23614\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1630 \n",
      "update - q expected : mean : 0.0346 - sd : 0.0150 min-max -0.0197|0.0499\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0019 - sd : 0.0031 min-max -0.0127|0.0155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1635/2000  81% ETA:  0:00:17 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1640 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0148 min-max -0.0134|0.0504\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0030 min-max -0.0229|0.0096\n",
      "Episode 1640 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.440 || 0.118 seconds, mem : 23898\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1650 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0145 min-max -0.0127|0.0814\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0040 min-max -0.0125|0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1655/2000  82% ETA:  0:00:16 ||||||||||||||||||||||||||||||||       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1660 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0143 min-max -0.0141|0.0503\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0029 min-max -0.0274|0.0103\n",
      "Episode 1660 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.436 || 0.111 seconds, mem : 24182\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1675/2000  83% ETA:  0:00:15 |///////////////////////////////       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1670 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0153 min-max -0.0168|0.0506\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0029 min-max -0.0244|0.0112\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1680 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0160 min-max -0.0173|0.0698\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0029 min-max -0.0271|0.0093\n",
      "Episode 1680 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.431 || 0.095 seconds, mem : 24466\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1690 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0153 min-max -0.0227|0.0513\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0026 min-max -0.0133|0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1695/2000  84% ETA:  0:00:14 |--------------------------------      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1700 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0163 min-max -0.0262|0.0866\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0059 min-max -0.0402|0.1032\n",
      "Episode 1700 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.427 || 0.092 seconds, mem : 24750\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1710 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0169 min-max -0.0214|0.0499\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0030 min-max -0.0166|0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1715/2000  85% ETA:  0:00:13 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1720 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0145 min-max -0.0152|0.0502\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0026 min-max -0.0146|0.0101\n",
      "Episode 1720 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.423 || 0.100 seconds, mem : 25034\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1730 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0145 min-max -0.0155|0.0494\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0054 min-max -0.0164|0.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1735/2000  86% ETA:  0:00:12 |||||||||||||||||||||||||||||||||      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1740 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0150 min-max -0.0136|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0030 min-max -0.0178|0.0100\n",
      "Episode 1740 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.419 || 0.087 seconds, mem : 25318\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1750 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0149 min-max -0.0131|0.0506\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0028 min-max -0.0115|0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1755/2000  87% ETA:  0:00:11 |/////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1760 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0147 min-max -0.0154|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0025 min-max -0.0184|0.0103\n",
      "Episode 1760 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.414 || 0.089 seconds, mem : 25602\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1770 \n",
      "update - q expected : mean : 0.0348 - sd : 0.0164 min-max -0.0195|0.0498\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0029 min-max -0.0180|0.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1775/2000  88% ETA:  0:00:10 |---------------------------------     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1780 \n",
      "update - q expected : mean : 0.0345 - sd : 0.0173 min-max -0.0238|0.0544\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0035 min-max -0.0359|0.0138\n",
      "Episode 1780 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.410 || 0.092 seconds, mem : 25886\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1790 \n",
      "update - q expected : mean : 0.0343 - sd : 0.0165 min-max -0.0216|0.0501\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0008 - sd : 0.0025 min-max -0.0076|0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1795/2000  89% ETA:  0:00:09 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1800 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0151 min-max -0.0232|0.0492\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0008 - sd : 0.0024 min-max -0.0096|0.0132\n",
      "Episode 1800 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.406 || 0.096 seconds, mem : 26170\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1815/2000  90% ETA:  0:00:08 |||||||||||||||||||||||||||||||||||    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1810 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0139 min-max -0.0068|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0017 - sd : 0.0037 min-max -0.0197|0.0098\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1820 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0145 min-max -0.0120|0.0504\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0034 min-max -0.0263|0.0103\n",
      "Episode 1820 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.402 || 0.088 seconds, mem : 26454\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1830 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0148 min-max -0.0154|0.0547\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0056 min-max -0.0264|0.1026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1835/2000  91% ETA:  0:00:07 |//////////////////////////////////    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1840 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0150 min-max -0.0164|0.0517\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0022 min-max -0.0109|0.0074\n",
      "Episode 1840 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.398 || 0.097 seconds, mem : 26738\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1850 \n",
      "update - q expected : mean : 0.0340 - sd : 0.0165 min-max -0.0238|0.0499\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0017 - sd : 0.0030 min-max -0.0088|0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1855/2000  92% ETA:  0:00:06 |-----------------------------------   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1860 \n",
      "update - q expected : mean : 0.0346 - sd : 0.0165 min-max -0.0249|0.0492\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0011 - sd : 0.0028 min-max -0.0106|0.0149\n",
      "Episode 1860 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.394 || 0.097 seconds, mem : 27022\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1870 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0149 min-max -0.0142|0.0746\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0060 min-max -0.0238|0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1875/2000  93% ETA:  0:00:05 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1880 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0139 min-max -0.0094|0.0498\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0200|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0029 min-max -0.0151|0.0085\n",
      "Episode 1880 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.390 || 0.097 seconds, mem : 27306\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1895/2000  94% ETA:  0:00:04 |||||||||||||||||||||||||||||||||||||  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1890 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0147 min-max -0.0162|0.0502\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0048 min-max -0.0242|0.0852\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1900 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0159 min-max -0.0202|0.0511\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0026 min-max -0.0135|0.0107\n",
      "Episode 1900 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.386 || 0.093 seconds, mem : 27590\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1910 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0159 min-max -0.0175|0.0534\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0023 min-max -0.0117|0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1915/2000  95% ETA:  0:00:04 |////////////////////////////////////  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1920 \n",
      "update - q expected : mean : 0.0355 - sd : 0.0156 min-max -0.0160|0.0501\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0026 min-max -0.0120|0.0099\n",
      "Episode 1920 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.383 || 0.091 seconds, mem : 27874\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1935/2000  96% ETA:  0:00:03 |------------------------------------  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1930 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0165 min-max -0.0176|0.0508\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0027 min-max -0.0142|0.0102\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1940 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0150 min-max -0.0189|0.0500\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0024 min-max -0.0153|0.0089\n",
      "Episode 1940 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.379 || 0.103 seconds, mem : 28158\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1955/2000  97% ETA:  0:00:02 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1950 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0157 min-max -0.0126|0.0509\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0023 min-max -0.0174|0.0106\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1960 \n",
      "update - q expected : mean : 0.0354 - sd : 0.0150 min-max -0.0177|0.0496\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0046 min-max -0.0104|0.0884\n",
      "Episode 1960 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.375 || 0.113 seconds, mem : 28442\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1970 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0149 min-max -0.0168|0.0567\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0025 min-max -0.0179|0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1975/2000  98% ETA:  0:00:01 |||||||||||||||||||||||||||||||||||||| | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1980 \n",
      "update - q expected : mean : 0.0347 - sd : 0.0163 min-max -0.0201|0.0581\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0025 min-max -0.0132|0.0101\n",
      "Episode 1980 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.371 || 0.105 seconds, mem : 28726\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1990 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0166 min-max -0.0190|0.0856\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0059 min-max -0.0165|0.0998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1995/2000  99% ETA:  0:00:00 |///////////////////////////////////// | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.01\n",
      "action_size          : 2\n",
      "state_size           : 24\n",
      "hidden_size          : 256\n",
      "buffer_size          : 50000\n",
      "batch_size           : 512\n",
      "dropout              : 0.01\n",
      "seed                 : 89\n",
      "max_episodes         : 2000\n",
      "learn_every          : 10\n",
      "critic_learning_rate : 0.001\n",
      "actor_learning_rate  : 0.001\n",
      "noise_decay          : 0.9995\n",
      "num_agents           : 2\n",
      "env_file_name        : Tennis_Windows_x86_64/Tennis.exe\n",
      "train_mode           : True\n",
      "brain_name           : TennisBrain\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ddpg_agent import ddpg_agent\n",
    "import cProfile\n",
    "DoProfile = False\n",
    "\n",
    "config = {\n",
    "    'gamma'               : 0.99,\n",
    "    'tau'                 : 0.01,\n",
    "    'action_size'         : action_size,\n",
    "    'state_size'          : state_size,\n",
    "    'hidden_size'         : 256,\n",
    "    'buffer_size'         : 50000,\n",
    "    'batch_size'          : 512,\n",
    "    'dropout'             : 0.01,\n",
    "    'seed'                : 88,\n",
    "    'max_episodes'        : 2000,\n",
    "    'learn_every'         : 10,\n",
    "    'critic_learning_rate': 1e-3,\n",
    "    'actor_learning_rate' : 1e-3,\n",
    "    'noise_decay'         : 0.9995,\n",
    "    'num_agents'          : num_agents,\n",
    "    'env_file_name'       : env_file_name,\n",
    "    'train_mode'          : True,\n",
    "    'brain_name'          : brain_name}\n",
    "\n",
    "def print_config(config):\n",
    "    print('Config Parameters    : ')\n",
    "    for c,k in config.items():\n",
    "        print('{:20s} : {}'.format(c,k))\n",
    "\n",
    "# seed_range = [0.1, 0.05, 0.03, 0.01, 0.005, 0.003, 0.001]\n",
    "for main in range(1):#len(tau_range)):\n",
    "    config['seed'] += 1\n",
    "    print_config(config)\n",
    "    agent = ddpg_agent(env, config)\n",
    "    if DoProfile:cProfile.run(\"results = agent.train()\",'PerfStats')\n",
    "    else:results = agent.train()\n",
    "    # all_rewards,avg_rewards,critic_losses,actor_losses = agent.train()\n",
    "    print_config(config)\n",
    "    plot_results(results)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
