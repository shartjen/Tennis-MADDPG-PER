{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env_file_name = \"Tennis_Windows_x86_64/Tennis.exe\"\n",
    "# env = UnityEnvironment(file_name=env_file_name)\n",
    "env = UnityEnvironment(file_name=env_file_name,no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "states shape :  (2, 24)\n",
      "Both states look like :  [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -13.30557251  -3.          -0.           0.\n",
      "   13.66344166  12.          -0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -12.93397141  -3.           0.           0.\n",
      "  -13.66344166  12.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('states shape : ',states.shape)\n",
    "print('Both states look like : ',states)\n",
    "print(2*states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    total_scores = []\n",
    "    for i in range(100):                                        # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t = 0\n",
    "        while True:\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            # print('actions : ',actions)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            t += 1\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}, and {} steps taken'.format(i, np.max(scores),t))\n",
    "        print(scores)\n",
    "        total_scores.append(scores)\n",
    "    print('Average Random Score : ', np.mean(total_scores))\n",
    "        \n",
    "def plot_results(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    plt.ion()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.all_rewards)), results.all_rewards)\n",
    "    plt.plot(np.arange(len(results.avg_rewards)), results.avg_rewards)\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.critic_loss)), results.critic_loss)\n",
    "    plt.ylabel('critic_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.actor_loss)), results.actor_loss)\n",
    "    plt.ylabel('actor_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/1000   0% ETA:  --:--:-- |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "New Run :\n",
      "-------------------------------------\n",
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.01\n",
      "action_size          : 2\n",
      "state_size           : 24\n",
      "hidden_size          : 512\n",
      "buffer_size          : 50000\n",
      "batch_size           : 512\n",
      "dropout              : 0.01\n",
      "seed                 : 150\n",
      "max_episodes         : 1000\n",
      "learn_every          : 10\n",
      "joined_states        : True\n",
      "critic_learning_rate : 0.001\n",
      "actor_learning_rate  : 0.001\n",
      "noise_decay          : 0.999\n",
      "sigma                : 0.1\n",
      "num_agents           : 2\n",
      "env_file_name        : Tennis_Windows_x86_64/Tennis.exe\n",
      "train_mode           : True\n",
      "brain_name           : TennisBrain\n",
      "Running on device :  cpu\n",
      "Episode 0 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.999 || 0.104 seconds, mem : 15\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "episode: 20/1000   2% ETA:  0:01:13 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.979 || 0.067 seconds, mem : 299\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 40/1000   4% ETA:  0:01:08 ||                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 40 \n",
      "update - q expected : mean : 0.0349 - sd : 0.0053 min-max 0.0029|0.0501\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0069 - sd : 0.0229 min-max -0.0941|0.0341\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 40 \n",
      "update - q expected : mean : 0.0349 - sd : 0.0055 min-max 0.0031|0.0496\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0069 - sd : 0.0230 min-max -0.0948|0.0349\n",
      "Episode 40 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.960 || 0.505 seconds, mem : 583\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 49/1000   4% ETA:  0:01:15 |/                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 50 \n",
      "update - q expected : mean : -0.0475 - sd : 0.0469 min-max -0.1731|0.0070\n",
      "update - reward : mean : -0.0008 - sd : 0.0028 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0737 - sd : 0.0473 min-max 0.0038|0.2157\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 50 \n",
      "update - q expected : mean : -0.0467 - sd : 0.0457 min-max -0.1710|0.0064\n",
      "update - reward : mean : -0.0008 - sd : 0.0028 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0730 - sd : 0.0462 min-max 0.0020|0.2122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 59/1000   5% ETA:  0:01:17 |--                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 60 \n",
      "update - q expected : mean : 0.0217 - sd : 0.0267 min-max -0.0287|0.0920\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0067 - sd : 0.0261 min-max -0.0632|0.0573\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 60 \n",
      "update - q expected : mean : 0.0223 - sd : 0.0269 min-max -0.0308|0.0986\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0060 - sd : 0.0264 min-max -0.0650|0.0568\n",
      "Episode 60 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.941 || 0.390 seconds, mem : 867\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 69/1000   6% ETA:  0:01:18 |\\\\                                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 70 \n",
      "update - q expected : mean : 0.0688 - sd : 0.0341 min-max 0.0136|0.1472\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0409 - sd : 0.0340 min-max -0.1286|0.0154\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 70 \n",
      "update - q expected : mean : 0.0685 - sd : 0.0343 min-max 0.0145|0.1516\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0406 - sd : 0.0342 min-max -0.1280|0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 79/1000   7% ETA:  0:01:17 ||||                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 80 \n",
      "update - q expected : mean : 0.0657 - sd : 0.0256 min-max 0.0277|0.1216\n",
      "update - reward : mean : -0.0009 - sd : 0.0029 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0395 - sd : 0.0277 min-max -0.1142|0.0025\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 80 \n",
      "update - q expected : mean : 0.0650 - sd : 0.0250 min-max 0.0282|0.1195\n",
      "update - reward : mean : -0.0009 - sd : 0.0029 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0388 - sd : 0.0273 min-max -0.1073|0.0014\n",
      "Episode 80 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.922 || 0.678 seconds, mem : 1151\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 85/1000   8% ETA:  0:01:23 |///                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 90 \n",
      "update - q expected : mean : 0.0513 - sd : 0.0141 min-max 0.0236|0.0862\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0230 - sd : 0.0209 min-max -0.0913|0.0038\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 90 \n",
      "update - q expected : mean : 0.0508 - sd : 0.0133 min-max 0.0236|0.0825\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0225 - sd : 0.0205 min-max -0.0901|0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 95/1000   9% ETA:  0:01:22 |---                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 100 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0081 min-max 0.0164|0.0612\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0098 - sd : 0.0201 min-max -0.0876|0.0090\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 100 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0076 min-max 0.0163|0.0593\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0097 - sd : 0.0196 min-max -0.0878|0.0124\n",
      "Episode 100 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.904 || 0.335 seconds, mem : 1435\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 105/1000  10% ETA:  0:01:21 |\\\\\\\\                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 110 \n",
      "update - q expected : mean : 0.0296 - sd : 0.0057 min-max 0.0134|0.0458\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0018 - sd : 0.0207 min-max -0.0786|0.0232\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 110 \n",
      "update - q expected : mean : 0.0293 - sd : 0.0057 min-max 0.0122|0.0454\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0014 - sd : 0.0206 min-max -0.0801|0.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 115/1000  11% ETA:  0:01:21 |||||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 120 \n",
      "update - q expected : mean : 0.0242 - sd : 0.0051 min-max 0.0075|0.0350\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0047 - sd : 0.0197 min-max -0.0765|0.0313\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 120 \n",
      "update - q expected : mean : 0.0239 - sd : 0.0054 min-max 0.0062|0.0319\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0051 - sd : 0.0194 min-max -0.0750|0.0359\n",
      "Episode 120 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.886 || 0.383 seconds, mem : 1719\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 125/1000  12% ETA:  0:01:21 |////                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 130 \n",
      "update - q expected : mean : 0.0212 - sd : 0.0047 min-max 0.0032|0.0296\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0072 - sd : 0.0196 min-max -0.0697|0.0313\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 130 \n",
      "update - q expected : mean : 0.0208 - sd : 0.0049 min-max 0.0021|0.0278\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0076 - sd : 0.0195 min-max -0.0701|0.0341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 135/1000  13% ETA:  0:01:20 |-----                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 140 \n",
      "update - q expected : mean : 0.0196 - sd : 0.0051 min-max 0.0007|0.0312\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0087 - sd : 0.0195 min-max -0.0713|0.0382\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 140 \n",
      "update - q expected : mean : 0.0192 - sd : 0.0052 min-max -0.0005|0.0274\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0091 - sd : 0.0194 min-max -0.0738|0.0355\n",
      "Episode 140 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.868 || 0.327 seconds, mem : 2003\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 145/1000  14% ETA:  0:01:19 |\\\\\\\\\\                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 150 \n",
      "update - q expected : mean : 0.0196 - sd : 0.0053 min-max -0.0015|0.0284\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0083 - sd : 0.0194 min-max -0.0716|0.0300\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 150 \n",
      "update - q expected : mean : 0.0192 - sd : 0.0056 min-max -0.0027|0.0280\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0086 - sd : 0.0192 min-max -0.0709|0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 155/1000  15% ETA:  0:01:19 |||||||                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 160 \n",
      "update - q expected : mean : 0.0199 - sd : 0.0060 min-max -0.0036|0.0274\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0078 - sd : 0.0186 min-max -0.0674|0.0315\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 160 \n",
      "update - q expected : mean : 0.0197 - sd : 0.0064 min-max -0.0048|0.0286\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0079 - sd : 0.0185 min-max -0.0674|0.0305\n",
      "Episode 160 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.851 || 0.355 seconds, mem : 2287\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 165/1000  16% ETA:  0:01:18 |//////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 170 \n",
      "update - q expected : mean : 0.0205 - sd : 0.0065 min-max -0.0064|0.0305\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0064 - sd : 0.0184 min-max -0.0728|0.0277\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 170 \n",
      "update - q expected : mean : 0.0203 - sd : 0.0069 min-max -0.0060|0.0300\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0066 - sd : 0.0181 min-max -0.0702|0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 175/1000  17% ETA:  0:01:17 |------                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 180 \n",
      "update - q expected : mean : 0.0214 - sd : 0.0072 min-max -0.0086|0.0300\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0044 - sd : 0.0182 min-max -0.0691|0.0249\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 180 \n",
      "update - q expected : mean : 0.0212 - sd : 0.0076 min-max -0.0105|0.0291\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0047 - sd : 0.0178 min-max -0.0683|0.0270\n",
      "Episode 180 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.834 || 0.307 seconds, mem : 2571\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 185/1000  18% ETA:  0:01:16 |\\\\\\\\\\\\\\                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 190 \n",
      "update - q expected : mean : 0.0223 - sd : 0.0084 min-max -0.0147|0.0325\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0045 - sd : 0.0155 min-max -0.0657|0.0253\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 190 \n",
      "update - q expected : mean : 0.0221 - sd : 0.0089 min-max -0.0169|0.0320\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0047 - sd : 0.0152 min-max -0.0638|0.0285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 195/1000  19% ETA:  0:01:15 ||||||||                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 200 \n",
      "update - q expected : mean : 0.0230 - sd : 0.0097 min-max -0.0205|0.0324\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0041 - sd : 0.0143 min-max -0.0642|0.0295\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 200 \n",
      "update - q expected : mean : 0.0227 - sd : 0.0104 min-max -0.0228|0.0325\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0044 - sd : 0.0138 min-max -0.0608|0.0313\n",
      "Episode 200 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.818 || 0.288 seconds, mem : 2855\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 205/1000  20% ETA:  0:01:13 |///////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 210 \n",
      "update - q expected : mean : 0.0234 - sd : 0.0110 min-max -0.0191|0.0329\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0028 - sd : 0.0138 min-max -0.0587|0.0300\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 210 \n",
      "update - q expected : mean : 0.0231 - sd : 0.0115 min-max -0.0212|0.0331\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0030 - sd : 0.0133 min-max -0.0656|0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 215/1000  21% ETA:  0:01:13 |--------                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 220 \n",
      "update - q expected : mean : 0.0237 - sd : 0.0115 min-max -0.0211|0.0335\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0024 - sd : 0.0135 min-max -0.0615|0.0310\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 220 \n",
      "update - q expected : mean : 0.0236 - sd : 0.0118 min-max -0.0242|0.0338\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0026 - sd : 0.0132 min-max -0.0606|0.0312\n",
      "Episode 220 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.802 || 0.384 seconds, mem : 3139\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 225/1000  22% ETA:  0:01:12 |\\\\\\\\\\\\\\\\                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 230 \n",
      "update - q expected : mean : 0.0240 - sd : 0.0127 min-max -0.0255|0.0342\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0142 min-max -0.0575|0.0317\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 230 \n",
      "update - q expected : mean : 0.0239 - sd : 0.0129 min-max -0.0271|0.0343\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0139 min-max -0.0597|0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 235/1000  23% ETA:  0:01:11 ||||||||||                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 240 \n",
      "update - q expected : mean : 0.0246 - sd : 0.0143 min-max -0.0324|0.0359\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0120 min-max -0.0515|0.0383\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 240 \n",
      "update - q expected : mean : 0.0246 - sd : 0.0145 min-max -0.0331|0.0361\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0117 min-max -0.0526|0.0395\n",
      "Episode 240 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.786 || 0.323 seconds, mem : 3423\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 245/1000  24% ETA:  0:01:10 |/////////                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 250 \n",
      "update - q expected : mean : 0.0246 - sd : 0.0154 min-max -0.0396|0.0376\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0126 min-max -0.0516|0.0403\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 250 \n",
      "update - q expected : mean : 0.0246 - sd : 0.0153 min-max -0.0398|0.0365\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0123 min-max -0.0455|0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 255/1000  25% ETA:  0:01:09 |---------                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 260 \n",
      "update - q expected : mean : 0.0250 - sd : 0.0161 min-max -0.0454|0.0381\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0111 min-max -0.0504|0.0418\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 260 \n",
      "update - q expected : mean : 0.0251 - sd : 0.0160 min-max -0.0458|0.0382\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0009 - sd : 0.0108 min-max -0.0444|0.0441\n",
      "Episode 260 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.770 || 0.283 seconds, mem : 3707\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 265/1000  26% ETA:  0:01:08 |\\\\\\\\\\\\\\\\\\\\                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 270 \n",
      "update - q expected : mean : 0.0253 - sd : 0.0154 min-max -0.0329|0.0377\n",
      "update - reward : mean : -0.0008 - sd : 0.0028 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0036 - sd : 0.0132 min-max -0.0534|0.0382\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 270 \n",
      "update - q expected : mean : 0.0255 - sd : 0.0150 min-max -0.0316|0.0370\n",
      "update - reward : mean : -0.0008 - sd : 0.0028 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0038 - sd : 0.0131 min-max -0.0538|0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 275/1000  27% ETA:  0:01:07 |||||||||||                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 273 with 30 steps || Reward : [0.   0.09] || avg reward :  0.001 || Noise  0.760 || 0.119 seconds, mem : 3907\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 280 \n",
      "update - q expected : mean : 0.0258 - sd : 0.0158 min-max -0.0416|0.0399\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0036 - sd : 0.0110 min-max -0.0484|0.0299\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 280 \n",
      "update - q expected : mean : 0.0260 - sd : 0.0154 min-max -0.0403|0.0398\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0039 - sd : 0.0106 min-max -0.0490|0.0263\n",
      "Episode 280 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.001 || Noise  0.755 || 0.317 seconds, mem : 4006\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 285/1000  28% ETA:  0:01:06 |///////////                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 282 with 30 steps || Reward : [0.   0.09] || avg reward :  0.002 || Noise  0.753 || 0.142 seconds, mem : 4051\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 290 \n",
      "update - q expected : mean : 0.0264 - sd : 0.0150 min-max -0.0420|0.0391\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0042 - sd : 0.0119 min-max -0.0464|0.0932\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 290 \n",
      "update - q expected : mean : 0.0266 - sd : 0.0146 min-max -0.0418|0.0392\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0045 - sd : 0.0116 min-max -0.0501|0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 295/1000  29% ETA:  0:01:05 |-----------                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 300 \n",
      "update - q expected : mean : 0.0255 - sd : 0.0166 min-max -0.0486|0.0400\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0037 - sd : 0.0107 min-max -0.0452|0.0380\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 300 \n",
      "update - q expected : mean : 0.0256 - sd : 0.0166 min-max -0.0489|0.0402\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0038 - sd : 0.0106 min-max -0.0429|0.0422\n",
      "Episode 300 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.002 || Noise  0.740 || 0.338 seconds, mem : 4307\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 305/1000  30% ETA:  0:01:04 |\\\\\\\\\\\\\\\\\\\\\\                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 310 \n",
      "update - q expected : mean : 0.0253 - sd : 0.0174 min-max -0.0552|0.0388\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0045 - sd : 0.0094 min-max -0.0433|0.0346\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 310 \n",
      "update - q expected : mean : 0.0253 - sd : 0.0174 min-max -0.0538|0.0389\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0045 - sd : 0.0093 min-max -0.0397|0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 315/1000  31% ETA:  0:01:03 |||||||||||||                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 320 \n",
      "update - q expected : mean : 0.0246 - sd : 0.0184 min-max -0.0550|0.0375\n",
      "update - reward : mean : -0.0009 - sd : 0.0028 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0052 - sd : 0.0102 min-max -0.0728|0.0373\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 320 \n",
      "update - q expected : mean : 0.0245 - sd : 0.0189 min-max -0.0575|0.0378\n",
      "update - reward : mean : -0.0009 - sd : 0.0028 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0050 - sd : 0.0101 min-max -0.0739|0.0382\n",
      "Episode 320 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.002 || Noise  0.725 || 0.285 seconds, mem : 4591\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 325/1000  32% ETA:  0:01:02 |////////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 330 \n",
      "update - q expected : mean : 0.0239 - sd : 0.0186 min-max -0.0573|0.0375\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0030 - sd : 0.0117 min-max -0.0392|0.0928\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 330 \n",
      "update - q expected : mean : 0.0238 - sd : 0.0189 min-max -0.0598|0.0374\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0029 - sd : 0.0116 min-max -0.0336|0.0935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 335/1000  33% ETA:  0:01:01 |-------------                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 340 \n",
      "update - q expected : mean : 0.0233 - sd : 0.0198 min-max -0.0653|0.0373\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0033 - sd : 0.0100 min-max -0.0385|0.0431\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 340 \n",
      "update - q expected : mean : 0.0233 - sd : 0.0202 min-max -0.0686|0.0373\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0031 - sd : 0.0101 min-max -0.0359|0.0435\n",
      "Episode 340 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.002 || Noise  0.711 || 0.270 seconds, mem : 4875\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 345/1000  34% ETA:  0:01:00 |\\\\\\\\\\\\\\\\\\\\\\\\\\                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 350 \n",
      "update - q expected : mean : 0.0236 - sd : 0.0183 min-max -0.0613|0.0362\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0045 - sd : 0.0090 min-max -0.0354|0.0432\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 350 \n",
      "update - q expected : mean : 0.0235 - sd : 0.0185 min-max -0.0619|0.0365\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0043 - sd : 0.0090 min-max -0.0357|0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 355/1000  35% ETA:  0:00:59 ||||||||||||||                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 360 \n",
      "update - q expected : mean : 0.0237 - sd : 0.0174 min-max -0.0672|0.0359\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0026 - sd : 0.0087 min-max -0.0258|0.0479\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 360 \n",
      "update - q expected : mean : 0.0237 - sd : 0.0172 min-max -0.0653|0.0364\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0026 - sd : 0.0084 min-max -0.0265|0.0371\n",
      "Episode 360 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.002 || Noise  0.697 || 0.315 seconds, mem : 5160\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 365/1000  36% ETA:  0:00:58 |//////////////                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 370 \n",
      "update - q expected : mean : 0.0241 - sd : 0.0149 min-max -0.0430|0.0348\n",
      "update - reward : mean : -0.0006 - sd : 0.0023 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0040 - sd : 0.0079 min-max -0.0342|0.0325\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 370 \n",
      "update - q expected : mean : 0.0242 - sd : 0.0145 min-max -0.0403|0.0356\n",
      "update - reward : mean : -0.0006 - sd : 0.0023 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0040 - sd : 0.0080 min-max -0.0390|0.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 372/1000  37% ETA:  0:00:58 |--------------                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 380 \n",
      "update - q expected : mean : 0.0245 - sd : 0.0126 min-max -0.0428|0.0345\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0042 - sd : 0.0078 min-max -0.0426|0.0236\n",
      "learn : Next States : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 381/1000  38% ETA:  0:00:57 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 380 \n",
      "update - q expected : mean : 0.0246 - sd : 0.0124 min-max -0.0423|0.0351\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0043 - sd : 0.0078 min-max -0.0413|0.0263\n",
      "Episode 380 with 15 steps || Reward : [ 0.   -0.01] || avg reward :  0.001 || Noise  0.683 || 0.434 seconds, mem : 5444\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 390 \n",
      "update - q expected : mean : 0.0232 - sd : 0.0143 min-max -0.0471|0.0347\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0040 - sd : 0.0093 min-max -0.0698|0.0945\n",
      "learn : Next States :  torch.Size([512, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 391/1000  39% ETA:  0:00:57 ||||||||||||||||                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 1 and episode 390 \n",
      "update - q expected : mean : 0.0234 - sd : 0.0142 min-max -0.0464|0.0347\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0040 - sd : 0.0089 min-max -0.0675|0.0943\n",
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 400 \n",
      "update - q expected : mean : 0.0218 - sd : 0.0154 min-max -0.0510|0.0343\n",
      "update - reward : mean : -0.0010 - sd : 0.0029 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0054 - sd : 0.0086 min-max -0.0394|0.0257\n",
      "learn : Next States :  torch.Size([512, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 401/1000  40% ETA:  0:00:56 |///////////////                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 1 and episode 400 \n",
      "update - q expected : mean : 0.0220 - sd : 0.0154 min-max -0.0514|0.0346\n",
      "update - reward : mean : -0.0010 - sd : 0.0029 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0055 - sd : 0.0083 min-max -0.0393|0.0238\n",
      "Episode 400 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.670 || 0.448 seconds, mem : 5728\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 410 \n",
      "update - q expected : mean : 0.0211 - sd : 0.0154 min-max -0.0460|0.0334\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0034 - sd : 0.0072 min-max -0.0491|0.0271\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 410 \n",
      "update - q expected : mean : 0.0211 - sd : 0.0156 min-max -0.0467|0.0337\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0034 - sd : 0.0069 min-max -0.0480|0.0277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 411/1000  41% ETA:  0:00:55 |----------------                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 412 with 31 steps || Reward : [0.   0.09] || avg reward :  0.001 || Noise  0.662 || 0.151 seconds, mem : 5915\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 420 \n",
      "update - q expected : mean : 0.0192 - sd : 0.0173 min-max -0.0632|0.0329\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0082 min-max -0.0329|0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 421/1000  42% ETA:  0:00:55 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 420 \n",
      "update - q expected : mean : 0.0193 - sd : 0.0176 min-max -0.0624|0.0334\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0080 min-max -0.0312|0.0369\n",
      "Episode 420 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.001 || Noise  0.656 || 0.484 seconds, mem : 6029\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 430 \n",
      "update - q expected : mean : 0.0169 - sd : 0.0210 min-max -0.0701|0.0323\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0086 min-max -0.0194|0.0407\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 430 \n",
      "update - q expected : mean : 0.0169 - sd : 0.0214 min-max -0.0715|0.0329\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0086 min-max -0.0182|0.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 431/1000  43% ETA:  0:00:54 |||||||||||||||||                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 433 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.002 || Noise  0.648 || 0.169 seconds, mem : 6232\n",
      "\u001b[0m\u001b[41mEpisode 435 with 29 steps || Reward : [ 0.1  -0.01] || avg reward :  0.003 || Noise  0.646 || 0.142 seconds, mem : 6275\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 440 \n",
      "update - q expected : mean : 0.0174 - sd : 0.0189 min-max -0.0655|0.0315\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0076 min-max -0.0215|0.0375\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 440 \n",
      "update - q expected : mean : 0.0176 - sd : 0.0189 min-max -0.0653|0.0326\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0075 min-max -0.0231|0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 441/1000  44% ETA:  0:00:53 |/////////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 440 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.003 || Noise  0.643 || 0.337 seconds, mem : 6346\n",
      "\u001b[0m\u001b[41mEpisode 441 with 31 steps || Reward : [0.   0.09] || avg reward :  0.004 || Noise  0.643 || 0.172 seconds, mem : 6377\n",
      "\u001b[0m\u001b[41mEpisode 443 with 32 steps || Reward : [0.   0.09] || avg reward :  0.005 || Noise  0.641 || 0.165 seconds, mem : 6423\n",
      "\u001b[0m\u001b[41mEpisode 445 with 30 steps || Reward : [ 0.1  -0.01] || avg reward :  0.006 || Noise  0.640 || 0.158 seconds, mem : 6467\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 450 \n",
      "update - q expected : mean : 0.0173 - sd : 0.0171 min-max -0.0563|0.0318\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0085 min-max -0.0295|0.1066\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 451/1000  45% ETA:  0:00:53 |-----------------                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent 1 and episode 450 \n",
      "update - q expected : mean : 0.0174 - sd : 0.0172 min-max -0.0559|0.0325\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0083 min-max -0.0274|0.1069\n",
      "\u001b[41mEpisode 456 with 30 steps || Reward : [ 0.1  -0.01] || avg reward :  0.007 || Noise  0.633 || 0.146 seconds, mem : 6639\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 460 \n",
      "update - q expected : mean : 0.0184 - sd : 0.0151 min-max -0.0509|0.0316\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0018 - sd : 0.0087 min-max -0.0663|0.1050\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 460 \n",
      "update - q expected : mean : 0.0186 - sd : 0.0148 min-max -0.0488|0.0323\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0018 - sd : 0.0086 min-max -0.0682|0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 461/1000  46% ETA:  0:00:52 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 460 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.007 || Noise  0.631 || 0.340 seconds, mem : 6695\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 470/1000  47% ETA:  0:00:51 |||||||||||||||||||                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 470 \n",
      "update - q expected : mean : 0.0192 - sd : 0.0112 min-max -0.0412|0.0309\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0024 - sd : 0.0078 min-max -0.0537|0.0161\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 470 \n",
      "update - q expected : mean : 0.0194 - sd : 0.0113 min-max -0.0409|0.0319\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0024 - sd : 0.0079 min-max -0.0544|0.0171\n",
      "\u001b[41mEpisode 470 with 32 steps || Reward : [0.   0.09] || avg reward :  0.008 || Noise  0.624 || 0.696 seconds, mem : 6854\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 475/1000  47% ETA:  0:00:51 |//////////////////                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 474 with 31 steps || Reward : [ 0.1  -0.01] || avg reward :  0.009 || Noise  0.622 || 0.166 seconds, mem : 6927\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 480 \n",
      "update - q expected : mean : 0.0183 - sd : 0.0115 min-max -0.0388|0.0311\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0026 - sd : 0.0088 min-max -0.0640|0.0228\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 480 \n",
      "update - q expected : mean : 0.0185 - sd : 0.0115 min-max -0.0388|0.0320\n",
      "update - reward : mean : -0.0007 - sd : 0.0026 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0026 - sd : 0.0089 min-max -0.0659|0.0247\n",
      "Episode 480 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.009 || Noise  0.618 || 0.346 seconds, mem : 7012\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 485/1000  48% ETA:  0:00:51 |------------------                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 490 \n",
      "update - q expected : mean : 0.0174 - sd : 0.0120 min-max -0.0406|0.0301\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0016 - sd : 0.0088 min-max -0.0622|0.0141\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 490 \n",
      "update - q expected : mean : 0.0177 - sd : 0.0119 min-max -0.0405|0.0307\n",
      "update - reward : mean : -0.0007 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0016 - sd : 0.0091 min-max -0.0636|0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 494/1000  49% ETA:  0:00:50 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 492 with 31 steps || Reward : [0.   0.09] || avg reward :  0.010 || Noise  0.611 || 0.153 seconds, mem : 7199\n",
      "\u001b[0m\u001b[41mEpisode 494 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.011 || Noise  0.609 || 0.164 seconds, mem : 7245\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 500 \n",
      "update - q expected : mean : 0.0157 - sd : 0.0136 min-max -0.0538|0.0289\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0097 min-max -0.0573|0.1073\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 500 \n",
      "update - q expected : mean : 0.0158 - sd : 0.0138 min-max -0.0496|0.0307\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0096 min-max -0.0584|0.1081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 503/1000  50% ETA:  0:00:49 ||||||||||||||||||||                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.011 || Noise  0.606 || 0.339 seconds, mem : 7330\n",
      "\u001b[0m\u001b[41mEpisode 509 with 31 steps || Reward : [0.   0.09] || avg reward :  0.011 || Noise  0.600 || 0.155 seconds, mem : 7477\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 510 \n",
      "update - q expected : mean : 0.0145 - sd : 0.0144 min-max -0.0560|0.0297\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0096 min-max -0.0587|0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 511/1000  51% ETA:  0:00:48 |///////////////////                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 510 \n",
      "update - q expected : mean : 0.0147 - sd : 0.0147 min-max -0.0566|0.0309\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0097 min-max -0.0595|0.1022\n",
      "\u001b[41mEpisode 517 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.012 || Noise  0.596 || 0.155 seconds, mem : 7608\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 520 \n",
      "update - q expected : mean : 0.0134 - sd : 0.0150 min-max -0.0559|0.0299\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0020 - sd : 0.0094 min-max -0.0447|0.1015\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 520 \n",
      "update - q expected : mean : 0.0136 - sd : 0.0155 min-max -0.0560|0.0287\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0022 - sd : 0.0094 min-max -0.0454|0.1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 521/1000  52% ETA:  0:00:48 |--------------------                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 520 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.013 || Noise  0.594 || 0.464 seconds, mem : 7669\n",
      "\u001b[0m\u001b[41mEpisode 527 with 31 steps || Reward : [0.   0.09] || avg reward :  0.013 || Noise  0.590 || 0.150 seconds, mem : 7785\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 530 \n",
      "update - q expected : mean : 0.0126 - sd : 0.0160 min-max -0.0564|0.0291\n",
      "\u001b[42mupdate - reward : mean : -0.0007 - sd : 0.0053 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0008 - sd : 0.0096 min-max -0.0470|0.0990\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 530 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0163 min-max -0.0572|0.0296\n",
      "\u001b[42mupdate - reward : mean : -0.0007 - sd : 0.0053 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0008 - sd : 0.0095 min-max -0.0473|0.0992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 531/1000  53% ETA:  0:00:47 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 538 with 37 steps || Reward : [0.1  0.09] || avg reward :  0.012 || Noise  0.583 || 0.185 seconds, mem : 7983\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 540 \n",
      "update - q expected : mean : 0.0131 - sd : 0.0145 min-max -0.0673|0.0278\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0028 - sd : 0.0104 min-max -0.0557|0.1097\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 541/1000  54% ETA:  0:00:46 ||||||||||||||||||||||                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 and episode 540 \n",
      "update - q expected : mean : 0.0135 - sd : 0.0144 min-max -0.0657|0.0291\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0027 - sd : 0.0103 min-max -0.0572|0.1103\n",
      "Episode 540 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.012 || Noise  0.582 || 0.427 seconds, mem : 8012\n",
      "\u001b[0m\u001b[41mEpisode 542 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.013 || Noise  0.581 || 0.172 seconds, mem : 8058\n",
      "\u001b[0m\u001b[41mEpisode 545 with 30 steps || Reward : [ 0.1  -0.01] || avg reward :  0.012 || Noise  0.579 || 0.196 seconds, mem : 8117\n",
      "\u001b[0m\u001b[41mEpisode 548 with 32 steps || Reward : [0.   0.09] || avg reward :  0.013 || Noise  0.577 || 0.186 seconds, mem : 8177\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 550 \n",
      "update - q expected : mean : 0.0131 - sd : 0.0141 min-max -0.0528|0.0282\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0015 - sd : 0.0077 min-max -0.0589|0.0178\n",
      "learn : Next States : "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 551/1000  55% ETA:  0:00:45 |/////////////////////                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 550 \n",
      "update - q expected : mean : 0.0133 - sd : 0.0143 min-max -0.0524|0.0291\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0016 - sd : 0.0077 min-max -0.0593|0.0197\n",
      "\u001b[41mEpisode 551 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.014 || Noise  0.576 || 0.331 seconds, mem : 8238\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 560/1000  56% ETA:  0:00:45 |---------------------                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 557 with 32 steps || Reward : [0.   0.09] || avg reward :  0.013 || Noise  0.572 || 0.156 seconds, mem : 8341\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 560 \n",
      "update - q expected : mean : 0.0133 - sd : 0.0149 min-max -0.0617|0.0279\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0013 - sd : 0.0076 min-max -0.0526|0.0262\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 560 \n",
      "update - q expected : mean : 0.0137 - sd : 0.0150 min-max -0.0613|0.0292\n",
      "update - reward : mean : -0.0006 - sd : 0.0025 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0013 - sd : 0.0075 min-max -0.0518|0.0264\n",
      "Episode 560 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.013 || Noise  0.570 || 0.333 seconds, mem : 8387\n",
      "\u001b[0m\u001b[41mEpisode 564 with 32 steps || Reward : [0.   0.09] || avg reward :  0.014 || Noise  0.568 || 0.171 seconds, mem : 8462\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 569/1000  56% ETA:  0:00:44 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 569 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.015 || Noise  0.565 || 0.161 seconds, mem : 8552\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 570 \n",
      "update - q expected : mean : 0.0140 - sd : 0.0140 min-max -0.0572|0.0291\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0081 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0104 min-max -0.0508|0.1037\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 570 \n",
      "update - q expected : mean : 0.0144 - sd : 0.0140 min-max -0.0573|0.0298\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0081 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0104 min-max -0.0513|0.1043\n",
      "\u001b[41mEpisode 570 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.015 || Noise  0.565 || 0.796 seconds, mem : 8584\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 572/1000  57% ETA:  0:00:44 |||||||||||||||||||||||                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 571 with 30 steps || Reward : [0.   0.09] || avg reward :  0.016 || Noise  0.564 || 0.130 seconds, mem : 8614\n",
      "\u001b[0m\u001b[41mEpisode 572 with 32 steps || Reward : [0.   0.09] || avg reward :  0.017 || Noise  0.564 || 0.137 seconds, mem : 8646\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 580 \n",
      "update - q expected : mean : 0.0145 - sd : 0.0137 min-max -0.0550|0.0295\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0110 min-max -0.0483|0.1015\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 580 \n",
      "update - q expected : mean : 0.0150 - sd : 0.0137 min-max -0.0547|0.0301\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0112 min-max -0.0491|0.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 581/1000  58% ETA:  0:00:43 |//////////////////////                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 580 with 32 steps || Reward : [0.   0.09] || avg reward :  0.017 || Noise  0.559 || 0.363 seconds, mem : 8779\n",
      "\u001b[0m\u001b[41mEpisode 585 with 52 steps || Reward : [-0.01  0.1 ] || avg reward :  0.018 || Noise  0.556 || 0.243 seconds, mem : 8888\n",
      "\u001b[0m\u001b[41mEpisode 586 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.019 || Noise  0.556 || 0.137 seconds, mem : 8921\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 590 \n",
      "update - q expected : mean : 0.0151 - sd : 0.0133 min-max -0.0554|0.0289\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0104 min-max -0.0496|0.1018\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 590 \n",
      "update - q expected : mean : 0.0155 - sd : 0.0137 min-max -0.0556|0.0295\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0104 min-max -0.0564|0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 591/1000  59% ETA:  0:00:42 |-----------------------                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 594 with 32 steps || Reward : [0.   0.09] || avg reward :  0.018 || Noise  0.551 || 0.179 seconds, mem : 9052\n",
      "\u001b[0m\u001b[41mEpisode 599 with 31 steps || Reward : [ 0.1  -0.01] || avg reward :  0.019 || Noise  0.549 || 0.124 seconds, mem : 9139\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 600 \n",
      "update - q expected : mean : 0.0159 - sd : 0.0111 min-max -0.0432|0.0297\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0016 - sd : 0.0087 min-max -0.0503|0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 601/1000  60% ETA:  0:00:41 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 600 \n",
      "update - q expected : mean : 0.0164 - sd : 0.0112 min-max -0.0430|0.0301\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0018 - sd : 0.0088 min-max -0.0504|0.1013\n",
      "Episode 600 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.019 || Noise  0.548 || 0.320 seconds, mem : 9154\n",
      "\u001b[0m\u001b[41mEpisode 604 with 31 steps || Reward : [ 0.1  -0.01] || avg reward :  0.020 || Noise  0.546 || 0.132 seconds, mem : 9227\n",
      "\u001b[0m\u001b[41mEpisode 608 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.544 || 0.137 seconds, mem : 9303\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 610 \n",
      "update - q expected : mean : 0.0148 - sd : 0.0138 min-max -0.0629|0.0298\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0105 min-max -0.0525|0.0997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 611/1000  61% ETA:  0:00:40 ||||||||||||||||||||||||                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 610 \n",
      "update - q expected : mean : 0.0153 - sd : 0.0138 min-max -0.0643|0.0302\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0106 min-max -0.0538|0.1012\n",
      "\u001b[41mEpisode 612 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.542 || 0.165 seconds, mem : 9378\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 620 \n",
      "update - q expected : mean : 0.0138 - sd : 0.0150 min-max -0.0617|0.0295\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0093 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0123 min-max -0.0548|0.1001\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 621/1000  62% ETA:  0:00:39 |////////////////////////               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent 1 and episode 620 \n",
      "update - q expected : mean : 0.0143 - sd : 0.0152 min-max -0.0631|0.0304\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0093 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0123 min-max -0.0546|0.1003\n",
      "\u001b[41mEpisode 620 with 32 steps || Reward : [-0.01  0.1 ] || avg reward :  0.020 || Noise  0.537 || 0.434 seconds, mem : 9528\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 630 \n",
      "update - q expected : mean : 0.0140 - sd : 0.0139 min-max -0.0571|0.0294\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0082 min-max -0.0446|0.1000\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 630 \n",
      "update - q expected : mean : 0.0146 - sd : 0.0143 min-max -0.0583|0.0302\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0051 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0083 min-max -0.0465|0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 631/1000  63% ETA:  0:00:38 |------------------------               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 634 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.020 || Noise  0.530 || 0.157 seconds, mem : 9745\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 640 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0151 min-max -0.0564|0.0294\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0093 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0118 min-max -0.0557|0.1026\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 640 \n",
      "update - q expected : mean : 0.0133 - sd : 0.0152 min-max -0.0580|0.0303\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0093 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0117 min-max -0.0558|0.1010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 641/1000  64% ETA:  0:00:37 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 640 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.020 || Noise  0.527 || 0.476 seconds, mem : 9848\n",
      "\u001b[0m\u001b[41mEpisode 641 with 33 steps || Reward : [0.   0.09] || avg reward :  0.021 || Noise  0.526 || 0.155 seconds, mem : 9881\n",
      "\u001b[0m\u001b[41mEpisode 645 with 32 steps || Reward : [0.   0.09] || avg reward :  0.020 || Noise  0.524 || 0.184 seconds, mem : 9956\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 650 \n",
      "update - q expected : mean : 0.0123 - sd : 0.0163 min-max -0.0589|0.0288\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0069 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0090 min-max -0.0380|0.1063\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 650 \n",
      "update - q expected : mean : 0.0127 - sd : 0.0166 min-max -0.0582|0.0298\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0069 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0091 min-max -0.0375|0.1079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 651/1000  65% ETA:  0:00:36 ||||||||||||||||||||||||||              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 651 with 32 steps || Reward : [0.   0.09] || avg reward :  0.019 || Noise  0.521 || 0.167 seconds, mem : 10059\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 660 \n",
      "update - q expected : mean : 0.0122 - sd : 0.0158 min-max -0.0590|0.0295\n",
      "\u001b[42mupdate - reward : mean : -0.0006 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0091 min-max -0.0549|0.1032\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 660 \n",
      "update - q expected : mean : 0.0126 - sd : 0.0159 min-max -0.0593|0.0298\n",
      "\u001b[42mupdate - reward : mean : -0.0006 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0092 min-max -0.0546|0.1041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 661/1000  66% ETA:  0:00:35 |/////////////////////////              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 660 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.018 || Noise  0.516 || 0.301 seconds, mem : 10187\n",
      "\u001b[0m\u001b[41mEpisode 663 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.019 || Noise  0.515 || 0.234 seconds, mem : 10269\n",
      "\u001b[0m\u001b[41mEpisode 664 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.019 || Noise  0.514 || 0.182 seconds, mem : 10302\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 668/1000  66% ETA:  0:00:35 |--------------------------             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 665 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.020 || Noise  0.514 || 0.179 seconds, mem : 10334\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 670 \n",
      "update - q expected : mean : 0.0122 - sd : 0.0164 min-max -0.0653|0.0286\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0018 - sd : 0.0058 min-max -0.0355|0.0273\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 670 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0165 min-max -0.0645|0.0302\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0016 - sd : 0.0059 min-max -0.0392|0.0262\n",
      "\u001b[41mEpisode 670 with 30 steps || Reward : [0.   0.09] || avg reward :  0.019 || Noise  0.511 || 0.457 seconds, mem : 10421\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 676/1000  67% ETA:  0:00:34 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 674 with 35 steps || Reward : [0.1  0.09] || avg reward :  0.018 || Noise  0.509 || 0.141 seconds, mem : 10499\n",
      "\u001b[0m\u001b[41mEpisode 678 with 33 steps || Reward : [0.   0.09] || avg reward :  0.019 || Noise  0.507 || 0.140 seconds, mem : 10575\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 680 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0152 min-max -0.0586|0.0294\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0060 min-max -0.0380|0.0274\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 680 \n",
      "update - q expected : mean : 0.0133 - sd : 0.0153 min-max -0.0582|0.0308\n",
      "update - reward : mean : -0.0006 - sd : 0.0024 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0060 min-max -0.0382|0.0261\n",
      "Episode 680 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.018 || Noise  0.506 || 0.286 seconds, mem : 10603\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 686/1000  68% ETA:  0:00:33 |||||||||||||||||||||||||||             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 682 with 30 steps || Reward : [ 0.1  -0.01] || avg reward :  0.019 || Noise  0.505 || 0.133 seconds, mem : 10648\n",
      "\u001b[0m\u001b[41mEpisode 686 with 32 steps || Reward : [0.   0.09] || avg reward :  0.018 || Noise  0.503 || 0.136 seconds, mem : 10722\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 690 \n",
      "update - q expected : mean : 0.0135 - sd : 0.0141 min-max -0.0512|0.0300\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0093 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0111 min-max -0.0409|0.1030\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 690 \n",
      "update - q expected : mean : 0.0140 - sd : 0.0143 min-max -0.0508|0.0300\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0093 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0112 min-max -0.0435|0.1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 696/1000  69% ETA:  0:00:32 |///////////////////////////            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 694 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.018 || Noise  0.499 || 0.131 seconds, mem : 10853\n",
      "\u001b[0m\u001b[41mEpisode 699 with 32 steps || Reward : [0.   0.09] || avg reward :  0.018 || Noise  0.496 || 0.129 seconds, mem : 10942\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 700 \n",
      "update - q expected : mean : 0.0137 - sd : 0.0139 min-max -0.0528|0.0296\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0074 min-max -0.0446|0.0928\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 700 \n",
      "update - q expected : mean : 0.0141 - sd : 0.0141 min-max -0.0528|0.0299\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0074 min-max -0.0442|0.0926\n",
      "Episode 700 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.018 || Noise  0.496 || 0.319 seconds, mem : 10956\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 706/1000  70% ETA:  0:00:31 |---------------------------            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 710 \n",
      "update - q expected : mean : 0.0138 - sd : 0.0141 min-max -0.0522|0.0297\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0111 min-max -0.0433|0.1003\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 710 \n",
      "update - q expected : mean : 0.0143 - sd : 0.0144 min-max -0.0526|0.0302\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0110 min-max -0.0424|0.0994\n",
      "\u001b[41mEpisode 711 with 32 steps || Reward : [0.   0.09] || avg reward :  0.017 || Noise  0.490 || 0.177 seconds, mem : 11130\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 715/1000  71% ETA:  0:00:30 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 713 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.017 || Noise  0.490 || 0.159 seconds, mem : 11176\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 720 \n",
      "update - q expected : mean : 0.0143 - sd : 0.0127 min-max -0.0424|0.0298\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0070 min-max -0.0325|0.0985\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 720 \n",
      "update - q expected : mean : 0.0147 - sd : 0.0129 min-max -0.0424|0.0300\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0071 min-max -0.0356|0.0985\n",
      "Episode 720 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.016 || Noise  0.486 || 0.297 seconds, mem : 11294\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 725/1000  72% ETA:  0:00:29 |||||||||||||||||||||||||||||           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 726 with 30 steps || Reward : [ 0.1  -0.01] || avg reward :  0.017 || Noise  0.483 || 0.123 seconds, mem : 11395\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 730 \n",
      "update - q expected : mean : 0.0141 - sd : 0.0135 min-max -0.0418|0.0297\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0016 - sd : 0.0106 min-max -0.0433|0.0983\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 730 \n",
      "update - q expected : mean : 0.0145 - sd : 0.0136 min-max -0.0417|0.0301\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0016 - sd : 0.0106 min-max -0.0440|0.0982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 733/1000  73% ETA:  0:00:28 |////////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 731 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.018 || Noise  0.481 || 0.147 seconds, mem : 11484\n",
      "\u001b[0m\u001b[41mEpisode 732 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.019 || Noise  0.480 || 0.170 seconds, mem : 11517\n",
      "\u001b[0m\u001b[41mEpisode 735 with 33 steps || Reward : [0.   0.09] || avg reward :  0.019 || Noise  0.479 || 0.146 seconds, mem : 11578\n",
      "\u001b[0m\u001b[41mEpisode 736 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.020 || Noise  0.478 || 0.139 seconds, mem : 11610\n",
      "\u001b[0m\u001b[41mEpisode 738 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.477 || 0.140 seconds, mem : 11658\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 740 \n",
      "update - q expected : mean : 0.0135 - sd : 0.0141 min-max -0.0403|0.0296\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0097 min-max -0.0523|0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 741/1000  74% ETA:  0:00:27 |----------------------------           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 740 \n",
      "update - q expected : mean : 0.0140 - sd : 0.0141 min-max -0.0410|0.0301\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0098 min-max -0.0510|0.0992\n",
      "Episode 740 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.020 || Noise  0.476 || 0.304 seconds, mem : 11686\n",
      "\u001b[0m\u001b[41mEpisode 745 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.019 || Noise  0.474 || 0.134 seconds, mem : 11776\n",
      "\u001b[0m\u001b[41mEpisode 746 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.020 || Noise  0.474 || 0.132 seconds, mem : 11808\n",
      "\u001b[0m\u001b[41mEpisode 748 with 26 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.473 || 0.132 seconds, mem : 11848\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 750 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0155 min-max -0.0516|0.0295\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0009 - sd : 0.0075 min-max -0.0347|0.1017\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 750 \n",
      "update - q expected : mean : 0.0134 - sd : 0.0152 min-max -0.0509|0.0300\n",
      "\u001b[42mupdate - reward : mean : -0.0005 - sd : 0.0052 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0011 - sd : 0.0076 min-max -0.0383|0.1013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 751/1000  75% ETA:  0:00:26 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 751 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.471 || 0.174 seconds, mem : 11909\n",
      "\u001b[0m\u001b[41mEpisode 756 with 32 steps || Reward : [0.   0.09] || avg reward :  0.022 || Noise  0.469 || 0.128 seconds, mem : 12002\n",
      "\u001b[0m\u001b[41mEpisode 758 with 31 steps || Reward : [0.   0.09] || avg reward :  0.023 || Noise  0.468 || 0.132 seconds, mem : 12047\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 760 \n",
      "update - q expected : mean : 0.0120 - sd : 0.0159 min-max -0.0558|0.0290\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0078 min-max -0.0461|0.0982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 761/1000  76% ETA:  0:00:25 ||||||||||||||||||||||||||||||          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 760 \n",
      "update - q expected : mean : 0.0125 - sd : 0.0158 min-max -0.0589|0.0299\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0078 min-max -0.0510|0.0974\n",
      "Episode 760 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.023 || Noise  0.467 || 0.313 seconds, mem : 12075\n",
      "\u001b[0m\u001b[41mEpisode 762 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.024 || Noise  0.466 || 0.146 seconds, mem : 12122\n",
      "\u001b[0m\u001b[41mEpisode 764 with 45 steps || Reward : [ 0.1  -0.01] || avg reward :  0.023 || Noise  0.465 || 0.203 seconds, mem : 12181\n",
      "\u001b[0m\u001b[41mEpisode 765 with 32 steps || Reward : [0.   0.09] || avg reward :  0.023 || Noise  0.465 || 0.139 seconds, mem : 12213\n",
      "\u001b[0m\u001b[41mEpisode 766 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.024 || Noise  0.464 || 0.147 seconds, mem : 12246\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 770 \n",
      "update - q expected : mean : 0.0115 - sd : 0.0157 min-max -0.0565|0.0296\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0089 min-max -0.0301|0.1066\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 770 \n",
      "update - q expected : mean : 0.0119 - sd : 0.0157 min-max -0.0575|0.0301\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0010 - sd : 0.0090 min-max -0.0348|0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 771/1000  77% ETA:  0:00:24 |//////////////////////////////         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 770 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.024 || Noise  0.462 || 0.365 seconds, mem : 12322\n",
      "\u001b[0m\u001b[41mEpisode 773 with 32 steps || Reward : [0.   0.09] || avg reward :  0.025 || Noise  0.461 || 0.140 seconds, mem : 12382\n",
      "\u001b[0m\u001b[41mEpisode 776 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.025 || Noise  0.460 || 0.146 seconds, mem : 12443\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 780 \n",
      "update - q expected : mean : 0.0113 - sd : 0.0164 min-max -0.0509|0.0295\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0059 min-max -0.0358|0.0184\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 780 \n",
      "update - q expected : mean : 0.0116 - sd : 0.0166 min-max -0.0524|0.0300\n",
      "update - reward : mean : -0.0008 - sd : 0.0027 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0060 min-max -0.0364|0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 781/1000  78% ETA:  0:00:23 |------------------------------         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 780 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.024 || Noise  0.458 || 0.291 seconds, mem : 12500\n",
      "\u001b[0m\u001b[41mEpisode 787 with 29 steps || Reward : [ 0.1  -0.01] || avg reward :  0.023 || Noise  0.455 || 0.124 seconds, mem : 12614\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 790 \n",
      "update - q expected : mean : 0.0117 - sd : 0.0139 min-max -0.0469|0.0301\n",
      "\u001b[42mupdate - reward : mean : 0.0010 - sd : 0.0127 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0018 - sd : 0.0136 min-max -0.0537|0.1028\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 790 \n",
      "update - q expected : mean : 0.0120 - sd : 0.0139 min-max -0.0478|0.0306\n",
      "\u001b[42mupdate - reward : mean : 0.0010 - sd : 0.0127 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0020 - sd : 0.0137 min-max -0.0538|0.1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 791/1000  79% ETA:  0:00:22 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 792 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.024 || Noise  0.452 || 0.143 seconds, mem : 12704\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 800 \n",
      "update - q expected : mean : 0.0118 - sd : 0.0142 min-max -0.0421|0.0296\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0089 min-max -0.0481|0.0978\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 800 \n",
      "update - q expected : mean : 0.0121 - sd : 0.0142 min-max -0.0438|0.0303\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0089 min-max -0.0474|0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 801/1000  80% ETA:  0:00:21 ||||||||||||||||||||||||||||||||        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 800 with 32 steps || Reward : [0.   0.09] || avg reward :  0.023 || Noise  0.449 || 0.370 seconds, mem : 12835\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 810 \n",
      "update - q expected : mean : 0.0122 - sd : 0.0146 min-max -0.0437|0.0295\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0084 min-max -0.0550|0.0950\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 810 \n",
      "update - q expected : mean : 0.0126 - sd : 0.0148 min-max -0.0442|0.0296\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0067 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0083 min-max -0.0550|0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 811/1000  81% ETA:  0:00:20 |///////////////////////////////        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 817 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.022 || Noise  0.441 || 0.160 seconds, mem : 13119\n",
      "\u001b[0m\u001b[41mEpisode 818 with 33 steps || Reward : [0.   0.09] || avg reward :  0.023 || Noise  0.441 || 0.161 seconds, mem : 13152\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 820 \n",
      "update - q expected : mean : 0.0123 - sd : 0.0140 min-max -0.0445|0.0296\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0097 min-max -0.0459|0.0984\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 820 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0140 min-max -0.0417|0.0299\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0097 min-max -0.0468|0.0984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 821/1000  82% ETA:  0:00:19 |--------------------------------       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 820 with 15 steps || Reward : [-0.01  0.  ] || avg reward :  0.023 || Noise  0.440 || 0.301 seconds, mem : 13181\n",
      "\u001b[0m\u001b[41mEpisode 821 with 30 steps || Reward : [0.   0.09] || avg reward :  0.024 || Noise  0.439 || 0.130 seconds, mem : 13211\n",
      "\u001b[0m\u001b[41mEpisode 824 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.025 || Noise  0.438 || 0.168 seconds, mem : 13272\n",
      "\u001b[0m\u001b[41mEpisode 826 with 34 steps || Reward : [0.   0.09] || avg reward :  0.025 || Noise  0.437 || 0.163 seconds, mem : 13326\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 831/1000  83% ETA:  0:00:18 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 830 \n",
      "update - q expected : mean : 0.0122 - sd : 0.0138 min-max -0.0396|0.0299\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0008 - sd : 0.0095 min-max -0.0483|0.1128\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 830 \n",
      "update - q expected : mean : 0.0125 - sd : 0.0140 min-max -0.0417|0.0300\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0095 min-max -0.0473|0.1159\n",
      "\u001b[41mEpisode 837 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.022 || Noise  0.432 || 0.137 seconds, mem : 13501\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 840 \n",
      "update - q expected : mean : 0.0119 - sd : 0.0151 min-max -0.0458|0.0298\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0092 min-max -0.0440|0.1001\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 840 \n",
      "update - q expected : mean : 0.0121 - sd : 0.0152 min-max -0.0484|0.0300\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0080 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0091 min-max -0.0423|0.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 841/1000  84% ETA:  0:00:16 |||||||||||||||||||||||||||||||||       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 840 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.021 || Noise  0.431 || 0.299 seconds, mem : 13543\n",
      "\u001b[0m\u001b[41mEpisode 841 with 33 steps || Reward : [-0.01  0.1 ] || avg reward :  0.022 || Noise  0.431 || 0.152 seconds, mem : 13576\n",
      "\u001b[0m\u001b[41mEpisode 842 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.023 || Noise  0.430 || 0.154 seconds, mem : 13609\n",
      "\u001b[0m\u001b[41mEpisode 843 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.024 || Noise  0.430 || 0.149 seconds, mem : 13642\n",
      "\u001b[0m\u001b[41mEpisode 845 with 32 steps || Reward : [0.   0.09] || avg reward :  0.024 || Noise  0.429 || 0.131 seconds, mem : 13688\n",
      "\u001b[0m\u001b[41mEpisode 847 with 33 steps || Reward : [ 0.1  -0.02] || avg reward :  0.024 || Noise  0.428 || 0.136 seconds, mem : 13735\n",
      "\u001b[0m\u001b[41mEpisode 849 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.024 || Noise  0.427 || 0.132 seconds, mem : 13782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 850/1000  85% ETA:  0:00:16 |/////////////////////////////////      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 850 \n",
      "update - q expected : mean : 0.0121 - sd : 0.0142 min-max -0.0416|0.0301\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0081 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0097 min-max -0.0539|0.1016\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 850 \n",
      "update - q expected : mean : 0.0126 - sd : 0.0144 min-max -0.0456|0.0303\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0081 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0098 min-max -0.0537|0.1013\n",
      "\u001b[41mEpisode 850 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.025 || Noise  0.427 || 0.432 seconds, mem : 13815\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 859/1000  85% ETA:  0:00:15 |---------------------------------      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 856 with 32 steps || Reward : [-0.01  0.1 ] || avg reward :  0.024 || Noise  0.424 || 0.134 seconds, mem : 13917\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 860 \n",
      "update - q expected : mean : 0.0120 - sd : 0.0149 min-max -0.0454|0.0297\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0083 min-max -0.0328|0.0950\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 860 \n",
      "update - q expected : mean : 0.0125 - sd : 0.0151 min-max -0.0456|0.0300\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0068 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0085 min-max -0.0371|0.0977\n",
      "Episode 860 with 14 steps || Reward : [ 0.   -0.01] || avg reward :  0.023 || Noise  0.423 || 0.310 seconds, mem : 13974\n",
      "\u001b[0m\u001b[41mEpisode 863 with 35 steps || Reward : [0.1  0.09] || avg reward :  0.023 || Noise  0.421 || 0.151 seconds, mem : 14042\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 869/1000  86% ETA:  0:00:13 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 869 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.419 || 0.145 seconds, mem : 14146\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 870 \n",
      "update - q expected : mean : 0.0121 - sd : 0.0145 min-max -0.0467|0.0298\n",
      "\u001b[42mupdate - reward : mean : 0.0005 - sd : 0.0101 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0017 - sd : 0.0108 min-max -0.0309|0.0986\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 870 \n",
      "update - q expected : mean : 0.0126 - sd : 0.0147 min-max -0.0466|0.0302\n",
      "\u001b[42mupdate - reward : mean : 0.0005 - sd : 0.0101 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0017 - sd : 0.0109 min-max -0.0320|0.0987\n",
      "\u001b[41mEpisode 870 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.021 || Noise  0.418 || 0.369 seconds, mem : 14178\n",
      "\u001b[0m\u001b[41mEpisode 872 with 32 steps || Reward : [ 0.1  -0.01] || avg reward :  0.022 || Noise  0.418 || 0.138 seconds, mem : 14225\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 877/1000  87% ETA:  0:00:13 |||||||||||||||||||||||||||||||||||     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 876 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.022 || Noise  0.416 || 0.149 seconds, mem : 14301\n",
      "\u001b[0m\u001b[41mEpisode 877 with 33 steps || Reward : [ 0.1  -0.01] || avg reward :  0.023 || Noise  0.415 || 0.160 seconds, mem : 14334\n",
      "\u001b[0mLearning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 880 \n",
      "update - q expected : mean : 0.0122 - sd : 0.0142 min-max -0.0484|0.0306\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0091 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0010 - sd : 0.0104 min-max -0.0390|0.1008\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 880 \n",
      "update - q expected : mean : 0.0128 - sd : 0.0143 min-max -0.0498|0.0306\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0091 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0105 min-max -0.0396|0.1016\n",
      "Episode 880 with 14 steps || Reward : [-0.01  0.  ] || avg reward :  0.023 || Noise  0.414 || 0.297 seconds, mem : 14376\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 887/1000  88% ETA:  0:00:12 |//////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning shape :  torch.Size([512, 48]) torch.Size([512, 4]) torch.Size([512, 1]) torch.Size([512, 48]) torch.Size([512, 1])\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 0 and episode 890 \n",
      "update - q expected : mean : 0.0126 - sd : 0.0147 min-max -0.0453|0.0300\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0081 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0091 min-max -0.0272|0.0974\n",
      "learn : Next States :  torch.Size([512, 48])\n",
      "--------------------------------------\n",
      "Agent 1 and episode 890 \n",
      "update - q expected : mean : 0.0133 - sd : 0.0147 min-max -0.0453|0.0304\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0081 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0091 min-max -0.0268|0.0983\n"
     ]
    }
   ],
   "source": [
    "from maddpg import maddpg\n",
    "import cProfile\n",
    "DoProfile = False\n",
    "\n",
    "config = {\n",
    "    'gamma'               : 0.99,\n",
    "    'tau'                 : 0.01,\n",
    "    'action_size'         : action_size,\n",
    "    'state_size'          : state_size,\n",
    "    'hidden_size'         : 512,\n",
    "    'buffer_size'         : 50000,\n",
    "    'batch_size'          : 512,\n",
    "    'dropout'             : 0.01,\n",
    "    'seed'                : 149,\n",
    "    'max_episodes'        : 1000,\n",
    "    'learn_every'         : 10,\n",
    "    'joined_states'       : True,\n",
    "    'critic_learning_rate': 1e-3,\n",
    "    'actor_learning_rate' : 1e-3,\n",
    "    'noise_decay'         : 0.999,\n",
    "    'sigma'               : 0.1,\n",
    "    'num_agents'          : num_agents,\n",
    "    'env_file_name'       : env_file_name,\n",
    "    'train_mode'          : True,\n",
    "    'brain_name'          : brain_name}\n",
    "\n",
    "def print_config(config):\n",
    "    print('Config Parameters    : ')\n",
    "    for c,k in config.items():\n",
    "        print('{:20s} : {}'.format(c,k))\n",
    "\n",
    "config_list = []\n",
    "result_list = []\n",
    "var_range = [0.0001] #, 0.0003, 0.0005, 0.001]\n",
    "num_runs = 5\n",
    "for param in range(len(var_range)):\n",
    "    alt_config = config.copy()\n",
    "    # alt_config['actor_learning_rate'] = var_range[param]\n",
    "    # alt_config['tau'] = config['tau']*curmult\n",
    "    # alt_config['critic_learning_rate'] = config['critic_learning_rate']*curmult\n",
    "    # alt_config['actor_learning_rate'] = config['actor_learning_rate']*curmult\n",
    "    for main in range(num_runs):#len(tau_range)):\n",
    "        print('-------------------------------------')\n",
    "        print('New Run :')\n",
    "        print('-------------------------------------')\n",
    "        alt_config['seed'] += 1\n",
    "        print_config(alt_config)\n",
    "        config_list.append(alt_config.copy())\n",
    "        agent = maddpg(env, alt_config)\n",
    "        if DoProfile:cProfile.run(\"results = agent.train()\",'PerfStats')\n",
    "        else:results = agent.train()\n",
    "        result_list.append(results)\n",
    "        # all_rewards,avg_rewards,critic_losses,actor_losses = agent.train()\n",
    "        print_config(alt_config)\n",
    "        plot_results(results)\n",
    "print('-------------------------------------')\n",
    "print('-------------------------------------')\n",
    "print('Summary :')\n",
    "print('-------------------------------------')\n",
    "print('-------------------------------------')\n",
    "for param in range(len(var_range)):\n",
    "    for main in range(num_runs):\n",
    "        print_config(config_list[param*num_runs+main])\n",
    "        plot_results(result_list[param*num_runs+main])\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
