{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env_file_name = \"Tennis_Windows_x86_64/Tennis.exe\"\n",
    "# env = UnityEnvironment(file_name=env_file_name)\n",
    "env = UnityEnvironment(file_name=env_file_name,no_graphics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "states shape :  (2, 24)\n",
      "Both states look like :  [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -13.30557251  -3.          -0.           0.\n",
      "   13.66344166  12.          -0.           0.        ]\n",
      " [  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.\n",
      "    0.         -12.93397141  -3.           0.           0.\n",
      "  -13.66344166  12.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('states shape : ',states.shape)\n",
    "print('Both states look like : ',states)\n",
    "print(2*states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    total_scores = []\n",
    "    for i in range(100):                                        # play game for 5 episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        t = 0\n",
    "        while True:\n",
    "            actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            # print('actions : ',actions)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            t += 1\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            states = next_states                               # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "        print('Score (max over agents) from episode {}: {}, and {} steps taken'.format(i, np.max(scores),t))\n",
    "        print(scores)\n",
    "        total_scores.append(scores)\n",
    "    print('Average Random Score : ', np.mean(total_scores))\n",
    "        \n",
    "def plot_results(results):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import torch\n",
    "    plt.ion()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.all_rewards)), results.all_rewards)\n",
    "    plt.plot(np.arange(len(results.avg_rewards)), results.avg_rewards)\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.critic_loss)), results.critic_loss)\n",
    "    plt.ylabel('critic_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(results.actor_loss)), results.actor_loss)\n",
    "    plt.ylabel('actor_losses')\n",
    "    plt.xlabel('Learn Step #')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 0/2000   0% ETA:  --:--:-- |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.01\n",
      "action_size          : 2\n",
      "state_size           : 24\n",
      "hidden_size          : 256\n",
      "buffer_size          : 50000\n",
      "batch_size           : 512\n",
      "dropout              : 0.01\n",
      "seed                 : 89\n",
      "max_episodes         : 2000\n",
      "learn_every          : 10\n",
      "critic_learning_rate : 0.001\n",
      "actor_learning_rate  : 0.001\n",
      "noise_decay          : 0.9995\n",
      "num_agents           : 2\n",
      "env_file_name        : Tennis_Windows_x86_64/Tennis.exe\n",
      "train_mode           : True\n",
      "brain_name           : TennisBrain\n",
      "Running on device :  cpu\n",
      "Episode 0 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  1.000 || 0.073 seconds, mem : 15\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "c:\\programdata\\anaconda3\\envs\\drlnd\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "episode: 13/2000   0% ETA:  0:02:40 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.990 || 0.064 seconds, mem : 299\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 32/2000   1% ETA:  0:02:06 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 40 \n",
      "update - q expected : mean : 0.0479 - sd : 0.0079 min-max 0.0217|0.0612\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0048 - sd : 0.0102 min-max -0.0493|0.0040\n",
      "--------------------------------------\n",
      "Agent 1 and episode 40 \n",
      "update - q expected : mean : 0.0478 - sd : 0.0076 min-max 0.0255|0.0618\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0054 - sd : 0.0113 min-max -0.0577|0.0051\n",
      "Episode 40 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.980 || 0.284 seconds, mem : 583\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 47/2000   2% ETA:  0:02:08 |                                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 50 \n",
      "update - q expected : mean : 0.0105 - sd : 0.0136 min-max -0.0359|0.0372\n",
      "update - reward : mean : -0.0002 - sd : 0.0013 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0328 - sd : 0.0117 min-max -0.0135|0.0561\n",
      "--------------------------------------\n",
      "Agent 1 and episode 50 \n",
      "update - q expected : mean : 0.0101 - sd : 0.0132 min-max -0.0490|0.0342\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0328 - sd : 0.0137 min-max -0.0333|0.0582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 63/2000   3% ETA:  0:02:06 ||                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 60 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0119 min-max -0.0040|0.0504\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0074 - sd : 0.0086 min-max -0.0342|0.0239\n",
      "--------------------------------------\n",
      "Agent 1 and episode 60 \n",
      "update - q expected : mean : 0.0343 - sd : 0.0112 min-max -0.0054|0.0482\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0077 - sd : 0.0093 min-max -0.0402|0.0269\n",
      "Episode 60 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.970 || 0.150 seconds, mem : 867\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 70 \n",
      "update - q expected : mean : 0.0547 - sd : 0.0137 min-max 0.0187|0.0785\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0126 - sd : 0.0103 min-max -0.0488|0.0021\n",
      "--------------------------------------\n",
      "Agent 1 and episode 70 \n",
      "update - q expected : mean : 0.0538 - sd : 0.0136 min-max 0.0188|0.0816\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0123 - sd : 0.0109 min-max -0.0572|0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 79/2000   3% ETA:  0:02:04 |/                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 80 \n",
      "update - q expected : mean : 0.0612 - sd : 0.0132 min-max 0.0325|0.0882\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0181 - sd : 0.0104 min-max -0.0561|-0.0010\n",
      "--------------------------------------\n",
      "Agent 1 and episode 80 \n",
      "update - q expected : mean : 0.0607 - sd : 0.0138 min-max 0.0271|0.0911\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0182 - sd : 0.0107 min-max -0.0617|0.0011\n",
      "Episode 80 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.960 || 0.158 seconds, mem : 1151\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 90 \n",
      "update - q expected : mean : 0.0595 - sd : 0.0115 min-max 0.0277|0.0814\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0172 - sd : 0.0104 min-max -0.0547|0.0005\n",
      "--------------------------------------\n",
      "Agent 1 and episode 90 \n",
      "update - q expected : mean : 0.0592 - sd : 0.0123 min-max 0.0292|0.0868\n",
      "update - reward : mean : -0.0005 - sd : 0.0023 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0177 - sd : 0.0111 min-max -0.0622|0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 95/2000   4% ETA:  0:02:03 |-                                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 95 || Reward : [0.1  0.09] || avg reward :  0.001 || Noise  0.953 || 0.135 seconds, mem : 1390\n",
      "\u001b[0m\u001b[41mEpisode 99 || Reward : [0.   0.09] || avg reward :  0.002 || Noise  0.951 || 0.118 seconds, mem : 1474\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 100 \n",
      "update - q expected : mean : 0.0547 - sd : 0.0092 min-max 0.0217|0.0705\n",
      "update - reward : mean : -0.0001 - sd : 0.0012 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0107 - sd : 0.0085 min-max -0.0482|0.0045\n",
      "--------------------------------------\n",
      "Agent 1 and episode 100 \n",
      "update - q expected : mean : 0.0544 - sd : 0.0109 min-max 0.0215|0.0734\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0113 - sd : 0.0091 min-max -0.0630|0.0117\n",
      "Episode 100 || Reward : [ 0.   -0.01] || avg reward :  0.002 || Noise  0.951 || 0.148 seconds, mem : 1488\n",
      "\u001b[0m\u001b[41mEpisode 101 || Reward : [-0.01  0.1 ] || avg reward :  0.003 || Noise  0.950 || 0.117 seconds, mem : 1520\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 107/2000   5% ETA:  0:02:07 |\\\\                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 105 || Reward : [ 0.1  -0.01] || avg reward :  0.004 || Noise  0.948 || 0.081 seconds, mem : 1602\n",
      "\u001b[0m\u001b[41mEpisode 106 || Reward : [0.   0.09] || avg reward :  0.005 || Noise  0.948 || 0.066 seconds, mem : 1621\n",
      "\u001b[0m\u001b[41mEpisode 107 || Reward : [-0.01  0.1 ] || avg reward :  0.006 || Noise  0.947 || 0.081 seconds, mem : 1644\n",
      "\u001b[0m\u001b[41mEpisode 109 || Reward : [ 0.1  -0.01] || avg reward :  0.007 || Noise  0.946 || 0.092 seconds, mem : 1685\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 110 \n",
      "update - q expected : mean : 0.0493 - sd : 0.0088 min-max 0.0221|0.0618\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0070 - sd : 0.0103 min-max -0.0622|0.0088\n",
      "--------------------------------------\n",
      "Agent 1 and episode 110 \n",
      "update - q expected : mean : 0.0490 - sd : 0.0105 min-max 0.0139|0.0639\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0092 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0064 - sd : 0.0137 min-max -0.0589|0.0986\n",
      "\u001b[41mEpisode 110 || Reward : [-0.01  0.1 ] || avg reward :  0.008 || Noise  0.946 || 0.198 seconds, mem : 1707\n",
      "\u001b[0m\u001b[41mEpisode 114 || Reward : [-0.01  0.1 ] || avg reward :  0.009 || Noise  0.944 || 0.083 seconds, mem : 1775\n",
      "\u001b[0m\u001b[41mEpisode 115 || Reward : [ 0.1  -0.01] || avg reward :  0.010 || Noise  0.944 || 0.076 seconds, mem : 1798\n",
      "\u001b[0m\u001b[41mEpisode 116 || Reward : [ 0.1  -0.01] || avg reward :  0.011 || Noise  0.943 || 0.073 seconds, mem : 1822\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 120/2000   6% ETA:  0:02:08 |||                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mEpisode 119 || Reward : [-0.01  0.1 ] || avg reward :  0.012 || Noise  0.942 || 0.078 seconds, mem : 1878\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 120 \n",
      "update - q expected : mean : 0.0444 - sd : 0.0094 min-max 0.0039|0.0549\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0108 min-max -0.0489|0.1087\n",
      "--------------------------------------\n",
      "Agent 1 and episode 120 \n",
      "update - q expected : mean : 0.0441 - sd : 0.0111 min-max 0.0048|0.0557\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0109 min-max -0.0454|0.1019\n",
      "Episode 120 || Reward : [-0.01  0.  ] || avg reward :  0.012 || Noise  0.941 || 0.158 seconds, mem : 1893\n",
      "\u001b[0m\u001b[41mEpisode 125 || Reward : [ 0.1  -0.01] || avg reward :  0.013 || Noise  0.939 || 0.089 seconds, mem : 1980\n",
      "\u001b[0m\u001b[41mEpisode 128 || Reward : [ 0.1  -0.01] || avg reward :  0.014 || Noise  0.938 || 0.086 seconds, mem : 2033\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 130 \n",
      "update - q expected : mean : 0.0405 - sd : 0.0104 min-max 0.0033|0.0523\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0032 - sd : 0.0112 min-max -0.0548|0.1048\n",
      "--------------------------------------\n",
      "Agent 1 and episode 130 \n",
      "update - q expected : mean : 0.0399 - sd : 0.0128 min-max -0.0093|0.0520\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0030 - sd : 0.0101 min-max -0.0508|0.1014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 134/2000   6% ETA:  0:02:08 |//                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 140 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0118 min-max -0.0123|0.0506\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0074 - sd : 0.0104 min-max -0.0482|0.1180\n",
      "--------------------------------------\n",
      "Agent 1 and episode 140 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0129 min-max -0.0128|0.0508\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0076 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0073 - sd : 0.0114 min-max -0.0411|0.1104\n",
      "Episode 140 || Reward : [-0.01  0.  ] || avg reward :  0.014 || Noise  0.932 || 0.160 seconds, mem : 2204\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 151/2000   7% ETA:  0:02:06 |--                                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 150 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0123 min-max -0.0125|0.0499\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0079 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0075 - sd : 0.0119 min-max -0.0479|0.1051\n",
      "--------------------------------------\n",
      "Agent 1 and episode 150 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0130 min-max -0.0195|0.0502\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0065 - sd : 0.0100 min-max -0.0396|0.0482\n",
      "--------------------------------------\n",
      "Agent 0 and episode 160 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0122 min-max -0.0149|0.0509\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0072 - sd : 0.0093 min-max -0.0374|0.0451\n",
      "--------------------------------------\n",
      "Agent 1 and episode 160 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0129 min-max -0.0196|0.0493\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0079 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0073 - sd : 0.0116 min-max -0.0406|0.1042\n",
      "Episode 160 || Reward : [ 0.   -0.01] || avg reward :  0.014 || Noise  0.923 || 0.153 seconds, mem : 2488\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 168/2000   8% ETA:  0:02:03 |\\\\\\                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 170 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0117 min-max -0.0071|0.0507\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0057 - sd : 0.0101 min-max -0.0378|0.1158\n",
      "--------------------------------------\n",
      "Agent 1 and episode 170 \n",
      "update - q expected : mean : 0.0382 - sd : 0.0115 min-max -0.0065|0.0500\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0050 - sd : 0.0099 min-max -0.0430|0.1002\n",
      "--------------------------------------\n",
      "Agent 0 and episode 180 \n",
      "update - q expected : mean : 0.0396 - sd : 0.0104 min-max -0.0006|0.0513\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0053 - sd : 0.0097 min-max -0.0351|0.1033\n",
      "--------------------------------------\n",
      "Agent 1 and episode 180 \n",
      "update - q expected : mean : 0.0401 - sd : 0.0099 min-max -0.0040|0.0501\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0046 - sd : 0.0100 min-max -0.0407|0.1082\n",
      "Episode 180 || Reward : [-0.01  0.  ] || avg reward :  0.014 || Noise  0.913 || 0.146 seconds, mem : 2772\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 184/2000   9% ETA:  0:02:01 ||||                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 190 \n",
      "update - q expected : mean : 0.0413 - sd : 0.0090 min-max 0.0076|0.0521\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0015 - sd : 0.0109 min-max -0.0381|0.1099\n",
      "--------------------------------------\n",
      "Agent 1 and episode 190 \n",
      "update - q expected : mean : 0.0417 - sd : 0.0086 min-max 0.0078|0.0511\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0013 - sd : 0.0122 min-max -0.0445|0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 198/2000   9% ETA:  0:02:01 |///                                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 200 \n",
      "update - q expected : mean : 0.0431 - sd : 0.0084 min-max 0.0055|0.0526\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0110 min-max -0.0381|0.0988\n",
      "--------------------------------------\n",
      "Agent 1 and episode 200 \n",
      "update - q expected : mean : 0.0431 - sd : 0.0081 min-max 0.0123|0.0517\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0105 min-max -0.0430|0.0986\n",
      "Episode 200 || Reward : [-0.01  0.  ] || avg reward :  0.012 || Noise  0.904 || 0.144 seconds, mem : 3056\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 210 \n",
      "update - q expected : mean : 0.0440 - sd : 0.0083 min-max 0.0095|0.0528\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0112 min-max -0.0554|0.1043\n",
      "--------------------------------------\n",
      "Agent 1 and episode 210 \n",
      "update - q expected : mean : 0.0439 - sd : 0.0076 min-max 0.0144|0.0519\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0011 - sd : 0.0105 min-max -0.0427|0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 214/2000  10% ETA:  0:01:59 |----                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 220 \n",
      "update - q expected : mean : 0.0450 - sd : 0.0080 min-max 0.0123|0.0534\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0025 - sd : 0.0104 min-max -0.0373|0.1027\n",
      "--------------------------------------\n",
      "Agent 1 and episode 220 \n",
      "update - q expected : mean : 0.0445 - sd : 0.0079 min-max 0.0148|0.0521\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0025 - sd : 0.0111 min-max -0.0462|0.0957\n",
      "Episode 220 || Reward : [ 0.   -0.01] || avg reward :  0.002 || Noise  0.895 || 0.165 seconds, mem : 3340\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 231/2000  11% ETA:  0:01:58 |\\\\\\\\                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 230 \n",
      "update - q expected : mean : 0.0453 - sd : 0.0085 min-max 0.0125|0.0536\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0019 - sd : 0.0101 min-max -0.0358|0.0948\n",
      "--------------------------------------\n",
      "Agent 1 and episode 230 \n",
      "update - q expected : mean : 0.0448 - sd : 0.0082 min-max 0.0150|0.0532\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0014 - sd : 0.0100 min-max -0.0397|0.0957\n",
      "--------------------------------------\n",
      "Agent 0 and episode 240 \n",
      "update - q expected : mean : 0.0454 - sd : 0.0090 min-max 0.0144|0.0546\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0017 - sd : 0.0090 min-max -0.0347|0.0945\n",
      "--------------------------------------\n",
      "Agent 1 and episode 240 \n",
      "update - q expected : mean : 0.0445 - sd : 0.0092 min-max 0.0164|0.0532\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0009 - sd : 0.0093 min-max -0.0388|0.1008\n",
      "Episode 240 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.886 || 0.147 seconds, mem : 3624\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 248/2000  12% ETA:  0:01:56 |||||                                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 250 \n",
      "update - q expected : mean : 0.0450 - sd : 0.0098 min-max 0.0122|0.0550\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0020 - sd : 0.0086 min-max -0.0362|0.0206\n",
      "--------------------------------------\n",
      "Agent 1 and episode 250 \n",
      "update - q expected : mean : 0.0440 - sd : 0.0100 min-max 0.0140|0.0533\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0014 - sd : 0.0086 min-max -0.0439|0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 262/2000  13% ETA:  0:01:56 |/////                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 260 \n",
      "update - q expected : mean : 0.0444 - sd : 0.0106 min-max 0.0090|0.0556\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0014 - sd : 0.0101 min-max -0.0420|0.0941\n",
      "--------------------------------------\n",
      "Agent 1 and episode 260 \n",
      "update - q expected : mean : 0.0435 - sd : 0.0112 min-max 0.0107|0.0538\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0083 min-max -0.0294|0.0228\n",
      "Episode 260 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.878 || 0.149 seconds, mem : 3908\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 270 \n",
      "update - q expected : mean : 0.0439 - sd : 0.0121 min-max 0.0084|0.0556\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0016 - sd : 0.0082 min-max -0.0553|0.0237\n",
      "--------------------------------------\n",
      "Agent 1 and episode 270 \n",
      "update - q expected : mean : 0.0429 - sd : 0.0127 min-max 0.0025|0.0540\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0096 min-max -0.0458|0.0975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 280/2000  14% ETA:  0:01:54 |-----                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 280 \n",
      "update - q expected : mean : 0.0435 - sd : 0.0125 min-max 0.0071|0.0555\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0013 - sd : 0.0089 min-max -0.0261|0.0946\n",
      "--------------------------------------\n",
      "Agent 1 and episode 280 \n",
      "update - q expected : mean : 0.0426 - sd : 0.0132 min-max 0.0011|0.0546\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0102 min-max -0.0328|0.1025\n",
      "Episode 280 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.869 || 0.163 seconds, mem : 4192\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 290 \n",
      "update - q expected : mean : 0.0428 - sd : 0.0134 min-max -0.0033|0.0552\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0084 min-max -0.0538|0.0235\n",
      "--------------------------------------\n",
      "Agent 1 and episode 290 \n",
      "update - q expected : mean : 0.0422 - sd : 0.0143 min-max -0.0059|0.0540\n",
      "\u001b[42mupdate - reward : mean : 0.0004 - sd : 0.0088 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0115 min-max -0.0438|0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 296/2000  14% ETA:  0:01:52 |\\\\\\\\\\                                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 300 \n",
      "update - q expected : mean : 0.0424 - sd : 0.0138 min-max -0.0002|0.0548\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0100 min-max -0.0533|0.0960\n",
      "--------------------------------------\n",
      "Agent 1 and episode 300 \n",
      "update - q expected : mean : 0.0421 - sd : 0.0140 min-max -0.0051|0.0541\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0091 min-max -0.0449|0.0975\n",
      "Episode 300 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.860 || 0.143 seconds, mem : 4476\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 312/2000  15% ETA:  0:01:51 |||||||                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 310 \n",
      "update - q expected : mean : 0.0422 - sd : 0.0143 min-max -0.0059|0.0545\n",
      "\u001b[42mupdate - reward : mean : 0.0004 - sd : 0.0078 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0010 - sd : 0.0106 min-max -0.0515|0.0983\n",
      "--------------------------------------\n",
      "Agent 1 and episode 310 \n",
      "update - q expected : mean : 0.0425 - sd : 0.0145 min-max -0.0076|0.0545\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0088 min-max -0.0433|0.1021\n",
      "--------------------------------------\n",
      "Agent 0 and episode 320 \n",
      "update - q expected : mean : 0.0421 - sd : 0.0143 min-max -0.0094|0.0543\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0082 min-max -0.0297|0.0218\n",
      "--------------------------------------\n",
      "Agent 1 and episode 320 \n",
      "update - q expected : mean : 0.0424 - sd : 0.0140 min-max -0.0123|0.0548\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0015 - sd : 0.0078 min-max -0.0265|0.0224\n",
      "Episode 320 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.852 || 0.189 seconds, mem : 4760\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 327/2000  16% ETA:  0:01:50 |//////                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 330 \n",
      "update - q expected : mean : 0.0421 - sd : 0.0142 min-max -0.0087|0.0541\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0078 min-max -0.0335|0.0243\n",
      "--------------------------------------\n",
      "Agent 1 and episode 330 \n",
      "update - q expected : mean : 0.0427 - sd : 0.0139 min-max -0.0062|0.0543\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0088 min-max -0.0316|0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 341/2000  17% ETA:  0:01:51 |------                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 340 \n",
      "update - q expected : mean : 0.0426 - sd : 0.0131 min-max -0.0014|0.0542\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0076 min-max -0.0404|0.0237\n",
      "--------------------------------------\n",
      "Agent 1 and episode 340 \n",
      "update - q expected : mean : 0.0430 - sd : 0.0127 min-max -0.0013|0.0544\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0062 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0087 min-max -0.0428|0.0994\n",
      "Episode 340 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.843 || 0.236 seconds, mem : 5044\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 350 \n",
      "update - q expected : mean : 0.0429 - sd : 0.0132 min-max -0.0042|0.0537\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0019 - sd : 0.0068 min-max -0.0340|0.0179\n",
      "--------------------------------------\n",
      "Agent 1 and episode 350 \n",
      "update - q expected : mean : 0.0435 - sd : 0.0124 min-max -0.0036|0.0543\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0021 - sd : 0.0079 min-max -0.0439|0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 354/2000  17% ETA:  0:01:50 |\\\\\\\\\\\\                                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 360 \n",
      "update - q expected : mean : 0.0433 - sd : 0.0127 min-max -0.0084|0.0534\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0012 - sd : 0.0091 min-max -0.0504|0.0980\n",
      "--------------------------------------\n",
      "Agent 1 and episode 360 \n",
      "update - q expected : mean : 0.0435 - sd : 0.0126 min-max -0.0022|0.0544\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0015 - sd : 0.0076 min-max -0.0433|0.0988\n",
      "Episode 360 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.835 || 0.157 seconds, mem : 5328\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 371/2000  18% ETA:  0:01:49 ||||||||                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 370 \n",
      "update - q expected : mean : 0.0430 - sd : 0.0120 min-max -0.0009|0.0536\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0018 - sd : 0.0087 min-max -0.0516|0.0969\n",
      "--------------------------------------\n",
      "Agent 1 and episode 370 \n",
      "update - q expected : mean : 0.0430 - sd : 0.0123 min-max 0.0074|0.0540\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0062 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0016 - sd : 0.0086 min-max -0.0437|0.1029\n",
      "--------------------------------------\n",
      "Agent 0 and episode 380 \n",
      "update - q expected : mean : 0.0430 - sd : 0.0120 min-max 0.0001|0.0535\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0022 - sd : 0.0067 min-max -0.0270|0.0218\n",
      "--------------------------------------\n",
      "Agent 1 and episode 380 \n",
      "update - q expected : mean : 0.0428 - sd : 0.0123 min-max 0.0049|0.0541\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0022 - sd : 0.0082 min-max -0.0332|0.0970\n",
      "Episode 380 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.827 || 0.146 seconds, mem : 5612\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 389/2000  19% ETA:  0:01:47 |///////                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 390 \n",
      "update - q expected : mean : 0.0424 - sd : 0.0128 min-max -0.0004|0.0531\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0060 min-max -0.0269|0.0259\n",
      "--------------------------------------\n",
      "Agent 1 and episode 390 \n",
      "update - q expected : mean : 0.0422 - sd : 0.0131 min-max -0.0024|0.0533\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0074 min-max -0.0246|0.0977\n",
      "--------------------------------------\n",
      "Agent 0 and episode 400 \n",
      "update - q expected : mean : 0.0422 - sd : 0.0124 min-max -0.0053|0.0531\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0089 min-max -0.0319|0.0963\n",
      "--------------------------------------\n",
      "Agent 1 and episode 400 \n",
      "update - q expected : mean : 0.0417 - sd : 0.0130 min-max -0.0097|0.0536\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0069 min-max -0.0253|0.0296\n",
      "Episode 400 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.818 || 0.146 seconds, mem : 5896\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 406/2000  20% ETA:  0:01:46 |-------                                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 410 \n",
      "update - q expected : mean : 0.0415 - sd : 0.0126 min-max -0.0035|0.0526\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0077 min-max -0.0249|0.0945\n",
      "--------------------------------------\n",
      "Agent 1 and episode 410 \n",
      "update - q expected : mean : 0.0414 - sd : 0.0130 min-max -0.0052|0.0530\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0075 min-max -0.0302|0.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 422/2000  21% ETA:  0:01:44 |\\\\\\\\\\\\\\\\                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 420 \n",
      "update - q expected : mean : 0.0412 - sd : 0.0130 min-max -0.0090|0.0528\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0064 min-max -0.0383|0.0326\n",
      "--------------------------------------\n",
      "Agent 1 and episode 420 \n",
      "update - q expected : mean : 0.0412 - sd : 0.0136 min-max -0.0109|0.0529\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0063 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0076 min-max -0.0257|0.0958\n",
      "Episode 420 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.810 || 0.139 seconds, mem : 6180\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 430 \n",
      "update - q expected : mean : 0.0411 - sd : 0.0130 min-max -0.0052|0.0524\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0070 min-max -0.0189|0.0938\n",
      "--------------------------------------\n",
      "Agent 1 and episode 430 \n",
      "update - q expected : mean : 0.0412 - sd : 0.0126 min-max -0.0090|0.0532\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0080 min-max -0.0218|0.0981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 440/2000  22% ETA:  0:01:42 |||||||||                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 440 \n",
      "update - q expected : mean : 0.0414 - sd : 0.0123 min-max -0.0066|0.0524\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0067 min-max -0.0199|0.0927\n",
      "--------------------------------------\n",
      "Agent 1 and episode 440 \n",
      "update - q expected : mean : 0.0415 - sd : 0.0120 min-max 0.0015|0.0527\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0052 min-max -0.0218|0.0267\n",
      "Episode 440 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.802 || 0.152 seconds, mem : 6464\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 450 \n",
      "update - q expected : mean : 0.0413 - sd : 0.0114 min-max -0.0015|0.0521\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0057 min-max -0.0325|0.0189\n",
      "--------------------------------------\n",
      "Agent 1 and episode 450 \n",
      "update - q expected : mean : 0.0414 - sd : 0.0117 min-max -0.0060|0.0527\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0086 min-max -0.0348|0.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 456/2000  22% ETA:  0:01:41 |////////                               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 460 \n",
      "update - q expected : mean : 0.0412 - sd : 0.0114 min-max -0.0061|0.0521\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0058 min-max -0.0282|0.0216\n",
      "--------------------------------------\n",
      "Agent 1 and episode 460 \n",
      "update - q expected : mean : 0.0413 - sd : 0.0113 min-max -0.0024|0.0523\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0053 min-max -0.0286|0.0207\n",
      "Episode 460 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.794 || 0.151 seconds, mem : 6748\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 472/2000  23% ETA:  0:01:40 |---------                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 470 \n",
      "update - q expected : mean : 0.0413 - sd : 0.0115 min-max -0.0031|0.0518\n",
      "\u001b[42mupdate - reward : mean : 0.0003 - sd : 0.0079 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0096 min-max -0.0319|0.1029\n",
      "--------------------------------------\n",
      "Agent 1 and episode 470 \n",
      "update - q expected : mean : 0.0414 - sd : 0.0119 min-max -0.0045|0.0522\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0056 min-max -0.0321|0.0201\n",
      "--------------------------------------\n",
      "Agent 0 and episode 480 \n",
      "update - q expected : mean : 0.0410 - sd : 0.0122 min-max -0.0118|0.0556\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0055 min-max -0.0395|0.0254\n",
      "--------------------------------------\n",
      "Agent 1 and episode 480 \n",
      "update - q expected : mean : 0.0408 - sd : 0.0125 min-max -0.0117|0.0525\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0044 min-max -0.0242|0.0192\n",
      "Episode 480 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.786 || 0.135 seconds, mem : 7032\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 490/2000  24% ETA:  0:01:38 |\\\\\\\\\\\\\\\\\\                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 490 \n",
      "update - q expected : mean : 0.0408 - sd : 0.0115 min-max -0.0038|0.0521\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0058 min-max -0.0273|0.0196\n",
      "--------------------------------------\n",
      "Agent 1 and episode 490 \n",
      "update - q expected : mean : 0.0403 - sd : 0.0122 min-max -0.0052|0.0523\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0051 min-max -0.0288|0.0181\n",
      "--------------------------------------\n",
      "Agent 0 and episode 500 \n",
      "update - q expected : mean : 0.0400 - sd : 0.0127 min-max -0.0042|0.0570\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0067 min-max -0.0227|0.0893\n",
      "--------------------------------------\n",
      "Agent 1 and episode 500 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0139 min-max -0.0170|0.0520\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0010 - sd : 0.0045 min-max -0.0186|0.0260\n",
      "Episode 500 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.778 || 0.155 seconds, mem : 7316\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 506/2000  25% ETA:  0:01:37 ||||||||||                              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 510 \n",
      "update - q expected : mean : 0.0399 - sd : 0.0120 min-max -0.0030|0.0512\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0075 min-max -0.0416|0.1001\n",
      "--------------------------------------\n",
      "Agent 1 and episode 510 \n",
      "update - q expected : mean : 0.0390 - sd : 0.0144 min-max -0.0214|0.0521\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0017 - sd : 0.0061 min-max -0.0107|0.0927\n",
      "\u001b[41mEpisode 513 || Reward : [ 0.1  -0.01] || avg reward :  0.001 || Noise  0.773 || 0.178 seconds, mem : 7538\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 521/2000  26% ETA:  0:01:37 |//////////                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 520 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0128 min-max -0.0069|0.0536\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0076 min-max -0.0529|0.0912\n",
      "--------------------------------------\n",
      "Agent 1 and episode 520 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0145 min-max -0.0156|0.0524\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0046 min-max -0.0214|0.0238\n",
      "Episode 520 || Reward : [-0.01  0.  ] || avg reward :  0.001 || Noise  0.771 || 0.159 seconds, mem : 7638\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 530 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0129 min-max -0.0089|0.0526\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0012 - sd : 0.0070 min-max -0.0463|0.0318\n",
      "--------------------------------------\n",
      "Agent 1 and episode 530 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0136 min-max -0.0129|0.0522\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0048 min-max -0.0157|0.0258\n",
      "\u001b[41mEpisode 531 || Reward : [0.1  0.09] || avg reward :  0.002 || Noise  0.766 || 0.172 seconds, mem : 7829\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 536/2000  26% ETA:  0:01:36 |----------                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 540 \n",
      "update - q expected : mean : 0.0392 - sd : 0.0129 min-max -0.0123|0.0551\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0013 - sd : 0.0072 min-max -0.0320|0.0881\n",
      "--------------------------------------\n",
      "Agent 1 and episode 540 \n",
      "update - q expected : mean : 0.0399 - sd : 0.0129 min-max -0.0115|0.0515\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0046 min-max -0.0241|0.0426\n",
      "Episode 540 || Reward : [-0.01  0.  ] || avg reward :  0.002 || Noise  0.763 || 0.142 seconds, mem : 7957\n",
      "\u001b[0m\u001b[41mEpisode 545 || Reward : [ 0.1  -0.01] || avg reward :  0.003 || Noise  0.761 || 0.111 seconds, mem : 8045\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 551/2000  27% ETA:  0:01:35 |\\\\\\\\\\\\\\\\\\\\                             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 550 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0129 min-max -0.0064|0.0539\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0064 min-max -0.0566|0.0276\n",
      "--------------------------------------\n",
      "Agent 1 and episode 550 \n",
      "update - q expected : mean : 0.0403 - sd : 0.0131 min-max -0.0119|0.0530\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0050 min-max -0.0402|0.0196\n",
      "\u001b[41mEpisode 550 || Reward : [ 0.1  -0.01] || avg reward :  0.004 || Noise  0.759 || 0.227 seconds, mem : 8131\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 560 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0130 min-max -0.0088|0.0516\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0058 min-max -0.0383|0.0255\n",
      "--------------------------------------\n",
      "Agent 1 and episode 560 \n",
      "update - q expected : mean : 0.0408 - sd : 0.0122 min-max -0.0023|0.0524\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0062 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0064 min-max -0.0250|0.0886\n",
      "Episode 560 || Reward : [ 0.   -0.01] || avg reward :  0.004 || Noise  0.755 || 0.147 seconds, mem : 8273\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 568/2000  28% ETA:  0:01:34 ||||||||||||                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 570 \n",
      "update - q expected : mean : 0.0400 - sd : 0.0122 min-max -0.0085|0.0607\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0080 min-max -0.0191|0.0922\n",
      "--------------------------------------\n",
      "Agent 1 and episode 570 \n",
      "update - q expected : mean : 0.0409 - sd : 0.0116 min-max -0.0047|0.0527\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0047 min-max -0.0301|0.0163\n",
      "--------------------------------------\n",
      "Agent 0 and episode 580 \n",
      "update - q expected : mean : 0.0405 - sd : 0.0120 min-max -0.0071|0.0545\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0070 min-max -0.0290|0.0964\n",
      "--------------------------------------\n",
      "Agent 1 and episode 580 \n",
      "update - q expected : mean : 0.0408 - sd : 0.0122 min-max -0.0147|0.0522\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0040 min-max -0.0234|0.0145\n",
      "Episode 580 || Reward : [ 0.   -0.01] || avg reward :  0.004 || Noise  0.748 || 0.162 seconds, mem : 8557\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 584/2000  29% ETA:  0:01:33 |///////////                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 590 \n",
      "update - q expected : mean : 0.0410 - sd : 0.0111 min-max -0.0009|0.0623\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0016 - sd : 0.0073 min-max -0.0331|0.0845\n",
      "--------------------------------------\n",
      "Agent 1 and episode 590 \n",
      "update - q expected : mean : 0.0406 - sd : 0.0119 min-max -0.0082|0.0532\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0011 - sd : 0.0063 min-max -0.0244|0.0914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 601/2000  30% ETA:  0:01:32 |-----------                            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 600 \n",
      "update - q expected : mean : 0.0409 - sd : 0.0110 min-max -0.0050|0.0571\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0066 min-max -0.0247|0.0857\n",
      "--------------------------------------\n",
      "Agent 1 and episode 600 \n",
      "update - q expected : mean : 0.0402 - sd : 0.0127 min-max -0.0234|0.0524\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0044 min-max -0.0181|0.0266\n",
      "Episode 600 || Reward : [-0.01  0.  ] || avg reward :  0.004 || Noise  0.740 || 0.145 seconds, mem : 8841\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 610 \n",
      "update - q expected : mean : 0.0403 - sd : 0.0124 min-max -0.0134|0.0526\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0071 min-max -0.0245|0.1022\n",
      "--------------------------------------\n",
      "Agent 1 and episode 610 \n",
      "update - q expected : mean : 0.0392 - sd : 0.0143 min-max -0.0264|0.0518\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0045 min-max -0.0407|0.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 618/2000  30% ETA:  0:01:31 |\\\\\\\\\\\\\\\\\\\\\\\\                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 620 \n",
      "update - q expected : mean : 0.0396 - sd : 0.0130 min-max -0.0120|0.0526\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0008 - sd : 0.0071 min-max -0.0393|0.0855\n",
      "--------------------------------------\n",
      "Agent 1 and episode 620 \n",
      "update - q expected : mean : 0.0390 - sd : 0.0147 min-max -0.0248|0.0520\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0062 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0065 min-max -0.0179|0.1016\n",
      "Episode 620 || Reward : [ 0.   -0.01] || avg reward :  0.003 || Noise  0.733 || 0.144 seconds, mem : 9125\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 630 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0138 min-max -0.0174|0.0602\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0074 min-max -0.0326|0.0811\n",
      "--------------------------------------\n",
      "Agent 1 and episode 630 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0132 min-max -0.0078|0.0516\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0013 - sd : 0.0048 min-max -0.0412|0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 634/2000  31% ETA:  0:01:30 |||||||||||||                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 640 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0147 min-max -0.0276|0.0571\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0015 - sd : 0.0065 min-max -0.0227|0.0390\n",
      "--------------------------------------\n",
      "Agent 1 and episode 640 \n",
      "update - q expected : mean : 0.0392 - sd : 0.0138 min-max -0.0210|0.0514\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0008 - sd : 0.0040 min-max -0.0193|0.0215\n",
      "Episode 640 || Reward : [-0.01  0.  ] || avg reward :  0.002 || Noise  0.726 || 0.154 seconds, mem : 9409\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 651/2000  32% ETA:  0:01:28 |////////////                           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 650 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0147 min-max -0.0251|0.0514\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0015 - sd : 0.0066 min-max -0.0145|0.0423\n",
      "--------------------------------------\n",
      "Agent 1 and episode 650 \n",
      "update - q expected : mean : 0.0393 - sd : 0.0135 min-max -0.0191|0.0523\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0060 min-max -0.0336|0.0984\n",
      "--------------------------------------\n",
      "Agent 0 and episode 660 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0135 min-max -0.0159|0.0537\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0050 min-max -0.0285|0.0207\n",
      "--------------------------------------\n",
      "Agent 1 and episode 660 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0133 min-max -0.0132|0.0528\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0036 min-max -0.0272|0.0136\n",
      "Episode 660 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.719 || 0.161 seconds, mem : 9693\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 669/2000  33% ETA:  0:01:27 |-------------                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 670 \n",
      "update - q expected : mean : 0.0401 - sd : 0.0114 min-max -0.0029|0.0539\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0058 min-max -0.0539|0.0235\n",
      "--------------------------------------\n",
      "Agent 1 and episode 670 \n",
      "update - q expected : mean : 0.0399 - sd : 0.0113 min-max -0.0078|0.0523\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0040 min-max -0.0314|0.0210\n",
      "--------------------------------------\n",
      "Agent 0 and episode 680 \n",
      "update - q expected : mean : 0.0407 - sd : 0.0109 min-max -0.0076|0.0563\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0015 - sd : 0.0084 min-max -0.0362|0.1025\n",
      "--------------------------------------\n",
      "Agent 1 and episode 680 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0129 min-max -0.0125|0.0518\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0034 min-max -0.0261|0.0112\n",
      "Episode 680 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.711 || 0.207 seconds, mem : 9977\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 684/2000  34% ETA:  0:01:26 |\\\\\\\\\\\\\\\\\\\\\\\\\\                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 690 \n",
      "update - q expected : mean : 0.0410 - sd : 0.0108 min-max -0.0116|0.0516\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0022 - sd : 0.0062 min-max -0.0530|0.0137\n",
      "--------------------------------------\n",
      "Agent 1 and episode 690 \n",
      "update - q expected : mean : 0.0392 - sd : 0.0132 min-max -0.0140|0.0508\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0056 min-max -0.0385|0.0951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 701/2000  35% ETA:  0:01:25 ||||||||||||||                          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 700 \n",
      "update - q expected : mean : 0.0406 - sd : 0.0106 min-max -0.0052|0.0514\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0051 min-max -0.0325|0.0213\n",
      "--------------------------------------\n",
      "Agent 1 and episode 700 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0138 min-max -0.0258|0.0518\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0033 min-max -0.0135|0.0220\n",
      "Episode 700 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.704 || 0.163 seconds, mem : 10261\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 710 \n",
      "update - q expected : mean : 0.0398 - sd : 0.0114 min-max -0.0107|0.0575\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0064 min-max -0.0303|0.0873\n",
      "--------------------------------------\n",
      "Agent 1 and episode 710 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0130 min-max -0.0221|0.0516\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0033 min-max -0.0201|0.0187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 719/2000  35% ETA:  0:01:24 |//////////////                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 720 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0127 min-max -0.0155|0.0508\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0050 min-max -0.0314|0.0219\n",
      "--------------------------------------\n",
      "Agent 1 and episode 720 \n",
      "update - q expected : mean : 0.0390 - sd : 0.0137 min-max -0.0226|0.0512\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0052 min-max -0.0109|0.0962\n",
      "Episode 720 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.697 || 0.149 seconds, mem : 10545\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 730 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0141 min-max -0.0171|0.0654\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0047 min-max -0.0250|0.0195\n",
      "--------------------------------------\n",
      "Agent 1 and episode 730 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0145 min-max -0.0240|0.0517\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0055 min-max -0.0213|0.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 736/2000  36% ETA:  0:01:22 |--------------                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 740 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0133 min-max -0.0079|0.0578\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0067 min-max -0.0285|0.0866\n",
      "--------------------------------------\n",
      "Agent 1 and episode 740 \n",
      "update - q expected : mean : 0.0390 - sd : 0.0130 min-max -0.0135|0.0519\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0041 min-max -0.0206|0.0322\n",
      "Episode 740 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.690 || 0.168 seconds, mem : 10829\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 752/2000  37% ETA:  0:01:21 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\                         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 750 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0135 min-max -0.0254|0.0662\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0023 - sd : 0.0068 min-max -0.0180|0.0921\n",
      "--------------------------------------\n",
      "Agent 1 and episode 750 \n",
      "update - q expected : mean : 0.0393 - sd : 0.0132 min-max -0.0191|0.0527\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0030 min-max -0.0183|0.0168\n",
      "--------------------------------------\n",
      "Agent 0 and episode 760 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0135 min-max -0.0168|0.0593\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0056 min-max -0.0343|0.0225\n",
      "--------------------------------------\n",
      "Agent 1 and episode 760 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0137 min-max -0.0121|0.0518\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0062 min-max -0.0495|0.1010\n",
      "Episode 760 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.683 || 0.128 seconds, mem : 11113\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 771/2000  38% ETA:  0:01:20 ||||||||||||||||                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 770 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0116 min-max -0.0102|0.0661\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0069 min-max -0.0289|0.0824\n",
      "--------------------------------------\n",
      "Agent 1 and episode 770 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0143 min-max -0.0188|0.0524\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0032 min-max -0.0210|0.0141\n",
      "--------------------------------------\n",
      "Agent 0 and episode 780 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0134 min-max -0.0254|0.0594\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0044 min-max -0.0205|0.0231\n",
      "--------------------------------------\n",
      "Agent 1 and episode 780 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0156 min-max -0.0276|0.0545\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0012 - sd : 0.0051 min-max -0.0128|0.0880\n",
      "Episode 780 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.677 || 0.142 seconds, mem : 11397\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 788/2000  39% ETA:  0:01:19 |///////////////                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 790 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0121 min-max -0.0086|0.0613\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0049 min-max -0.0197|0.0193\n",
      "--------------------------------------\n",
      "Agent 1 and episode 790 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0156 min-max -0.0272|0.0550\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0051 min-max -0.0134|0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 801/2000  40% ETA:  0:01:18 |---------------                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 800 \n",
      "update - q expected : mean : 0.0390 - sd : 0.0114 min-max -0.0121|0.0652\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0078 min-max -0.0377|0.1005\n",
      "--------------------------------------\n",
      "Agent 1 and episode 800 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0138 min-max -0.0167|0.0574\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0063 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0055 min-max -0.0201|0.0852\n",
      "Episode 800 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.670 || 0.249 seconds, mem : 11681\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 810 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0126 min-max -0.0188|0.0524\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0053 min-max -0.0319|0.0275\n",
      "--------------------------------------\n",
      "Agent 1 and episode 810 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0144 min-max -0.0202|0.0591\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0047 min-max -0.0167|0.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 815/2000  40% ETA:  0:01:17 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 820 \n",
      "update - q expected : mean : 0.0380 - sd : 0.0128 min-max -0.0177|0.0633\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0051 min-max -0.0238|0.0357\n",
      "--------------------------------------\n",
      "Agent 1 and episode 820 \n",
      "update - q expected : mean : 0.0387 - sd : 0.0141 min-max -0.0202|0.0526\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0032 min-max -0.0196|0.0146\n",
      "Episode 820 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.663 || 0.147 seconds, mem : 11965\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 831/2000  41% ETA:  0:01:16 |||||||||||||||||                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 830 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0151 min-max -0.0319|0.0531\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0019 - sd : 0.0069 min-max -0.0182|0.0892\n",
      "--------------------------------------\n",
      "Agent 1 and episode 830 \n",
      "update - q expected : mean : 0.0387 - sd : 0.0138 min-max -0.0157|0.0630\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0044 min-max -0.0167|0.0719\n",
      "--------------------------------------\n",
      "Agent 0 and episode 840 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0156 min-max -0.0237|0.0708\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0008 - sd : 0.0072 min-max -0.0348|0.0892\n",
      "--------------------------------------\n",
      "Agent 1 and episode 840 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0151 min-max -0.0178|0.0560\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0044 min-max -0.0100|0.0900\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0039 min-max -0.0206|0.0529\n",
      "Episode 840 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.657 || 0.149 seconds, mem : 12249\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 849/2000  42% ETA:  0:01:15 |////////////////                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 850 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0120 min-max -0.0169|0.0639\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0051 min-max -0.0249|0.0263\n",
      "--------------------------------------\n",
      "Agent 1 and episode 850 \n",
      "update - q expected : mean : 0.0399 - sd : 0.0112 min-max -0.0074|0.0707\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0011 - sd : 0.0042 min-max -0.0484|0.0087\n",
      "--------------------------------------\n",
      "Agent 0 and episode 860 \n",
      "update - q expected : mean : 0.0392 - sd : 0.0128 min-max -0.0186|0.0727\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0008 - sd : 0.0057 min-max -0.0248|0.0760\n",
      "--------------------------------------\n",
      "Agent 1 and episode 860 \n",
      "update - q expected : mean : 0.0387 - sd : 0.0136 min-max -0.0122|0.0659\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0032 min-max -0.0227|0.0115\n",
      "Episode 860 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.650 || 0.144 seconds, mem : 12533\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 866/2000  43% ETA:  0:01:14 |----------------                       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 870 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0114 min-max -0.0078|0.0603\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0007 - sd : 0.0065 min-max -0.0334|0.1007\n",
      "--------------------------------------\n",
      "Agent 1 and episode 870 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0138 min-max -0.0203|0.0517\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0056 min-max -0.0467|0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 882/2000  44% ETA:  0:01:12 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 880 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0116 min-max -0.0059|0.0664\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0055 min-max -0.0188|0.0797\n",
      "--------------------------------------\n",
      "Agent 1 and episode 880 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0142 min-max -0.0207|0.0654\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0010 - sd : 0.0032 min-max -0.0278|0.0160\n",
      "Episode 880 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.644 || 0.141 seconds, mem : 12817\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 890 \n",
      "update - q expected : mean : 0.0397 - sd : 0.0121 min-max -0.0132|0.0763\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0011 - sd : 0.0054 min-max -0.0272|0.0741\n",
      "--------------------------------------\n",
      "Agent 1 and episode 890 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0144 min-max -0.0219|0.0524\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0027 min-max -0.0146|0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 900/2000  45% ETA:  0:01:11 ||||||||||||||||||                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 900 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0127 min-max -0.0136|0.0746\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0053 min-max -0.0368|0.0679\n",
      "--------------------------------------\n",
      "Agent 1 and episode 900 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0135 min-max -0.0152|0.0519\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0044 min-max -0.0100|0.0900\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0035 min-max -0.0165|0.0568\n",
      "Episode 900 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.637 || 0.173 seconds, mem : 13101\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 910 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0128 min-max -0.0105|0.0693\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0014 - sd : 0.0052 min-max -0.0297|0.0165\n",
      "--------------------------------------\n",
      "Agent 1 and episode 910 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0129 min-max -0.0060|0.0524\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0016 - sd : 0.0040 min-max -0.0266|0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 914/2000  45% ETA:  0:01:10 |/////////////////                      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 920 \n",
      "update - q expected : mean : 0.0380 - sd : 0.0138 min-max -0.0244|0.0718\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0042 min-max -0.0265|0.0192\n",
      "--------------------------------------\n",
      "Agent 1 and episode 920 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0129 min-max -0.0103|0.0559\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0048 min-max -0.0198|0.0869\n",
      "Episode 920 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.631 || 0.135 seconds, mem : 13385\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 931/2000  46% ETA:  0:01:09 |------------------                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 930 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0145 min-max -0.0326|0.0510\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0014 - sd : 0.0052 min-max -0.0217|0.0307\n",
      "--------------------------------------\n",
      "Agent 1 and episode 930 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0148 min-max -0.0250|0.0521\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0012 - sd : 0.0029 min-max -0.0107|0.0178\n",
      "--------------------------------------\n",
      "Agent 0 and episode 940 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0142 min-max -0.0217|0.0586\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0045 min-max -0.0269|0.0200\n",
      "--------------------------------------\n",
      "Agent 1 and episode 940 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0136 min-max -0.0138|0.0527\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0053 min-max -0.0235|0.0884\n",
      "Episode 940 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.625 || 0.153 seconds, mem : 13669\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 949/2000  47% ETA:  0:01:08 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 950 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0141 min-max -0.0229|0.0530\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0044 min-max -0.0178|0.0180\n",
      "--------------------------------------\n",
      "Agent 1 and episode 950 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0152 min-max -0.0204|0.0505\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0031 min-max -0.0209|0.0125\n",
      "--------------------------------------\n",
      "Agent 0 and episode 960 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0135 min-max -0.0247|0.0562\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0012 - sd : 0.0046 min-max -0.0163|0.0258\n",
      "--------------------------------------\n",
      "Agent 1 and episode 960 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0150 min-max -0.0278|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0012 - sd : 0.0037 min-max -0.0207|0.0202\n",
      "Episode 960 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.618 || 0.162 seconds, mem : 13953\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 966/2000  48% ETA:  0:01:07 |||||||||||||||||||                     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 970 \n",
      "update - q expected : mean : 0.0384 - sd : 0.0117 min-max -0.0184|0.0717\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0046 min-max -0.0268|0.0162\n",
      "--------------------------------------\n",
      "Agent 1 and episode 970 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0135 min-max -0.0160|0.0547\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0049 min-max -0.0208|0.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 983/2000  49% ETA:  0:01:05 |///////////////////                    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 980 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0115 min-max -0.0207|0.0711\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0002 - sd : 0.0050 min-max -0.0162|0.0779\n",
      "--------------------------------------\n",
      "Agent 1 and episode 980 \n",
      "update - q expected : mean : 0.0389 - sd : 0.0132 min-max -0.0240|0.0599\n",
      "\u001b[42mupdate - reward : mean : 0.0002 - sd : 0.0064 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0010 - sd : 0.0063 min-max -0.0190|0.1005\n",
      "Episode 980 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.612 || 0.129 seconds, mem : 14237\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 990 \n",
      "update - q expected : mean : 0.0388 - sd : 0.0114 min-max -0.0127|0.0530\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0045 min-max -0.0356|0.0172\n",
      "--------------------------------------\n",
      "Agent 1 and episode 990 \n",
      "update - q expected : mean : 0.0394 - sd : 0.0120 min-max -0.0074|0.0611\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0043 min-max -0.0100|0.0900\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0013 - sd : 0.0045 min-max -0.0425|0.0530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1001/2000  50% ETA:  0:01:04 |-------------------                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1000 \n",
      "update - q expected : mean : 0.0391 - sd : 0.0114 min-max -0.0095|0.0795\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0006 - sd : 0.0071 min-max -0.0199|0.1028\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1000 \n",
      "update - q expected : mean : 0.0395 - sd : 0.0125 min-max -0.0073|0.0660\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0012 - sd : 0.0048 min-max -0.0244|0.0767\n",
      "Episode 1000 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.606 || 0.144 seconds, mem : 14521\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1010 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0120 min-max -0.0144|0.0528\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0015 - sd : 0.0048 min-max -0.0226|0.0103\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1010 \n",
      "update - q expected : mean : 0.0390 - sd : 0.0125 min-max -0.0060|0.0508\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0020 - sd : 0.0045 min-max -0.0285|0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1018/2000  50% ETA:  0:01:03 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1020 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0133 min-max -0.0213|0.0531\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0038 min-max -0.0186|0.0182\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1020 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0147 min-max -0.0165|0.0520\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0028 min-max -0.0204|0.0102\n",
      "Episode 1020 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.600 || 0.135 seconds, mem : 14805\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1030 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0139 min-max -0.0176|0.0751\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0059 min-max -0.0242|0.0864\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1030 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0147 min-max -0.0186|0.0529\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0036 min-max -0.0157|0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1032/2000  51% ETA:  0:01:02 ||||||||||||||||||||                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1040 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0167 min-max -0.0374|0.0809\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0016 - sd : 0.0057 min-max -0.0126|0.0710\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1040 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0169 min-max -0.0286|0.0523\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0014 - sd : 0.0037 min-max -0.0165|0.0186\n",
      "Episode 1040 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.594 || 0.163 seconds, mem : 15089\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1050/2000  52% ETA:  0:01:01 |///////////////////                   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1050 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0143 min-max -0.0254|0.0713\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0046 min-max -0.0258|0.0243\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1050 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0172 min-max -0.0296|0.0644\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0016 - sd : 0.0045 min-max -0.0231|0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1061/2000  53% ETA:  0:01:01 |--------------------                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1060 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0132 min-max -0.0197|0.0507\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0044 min-max -0.0283|0.0289\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1060 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0140 min-max -0.0176|0.0614\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0050 min-max -0.0268|0.0796\n",
      "Episode 1060 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.588 || 0.203 seconds, mem : 15373\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1070 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0133 min-max -0.0167|0.0683\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0009 - sd : 0.0055 min-max -0.0251|0.0754\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1070 \n",
      "update - q expected : mean : 0.0386 - sd : 0.0122 min-max -0.0045|0.0525\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0017 - sd : 0.0041 min-max -0.0242|0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1073/2000  53% ETA:  0:01:00 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1080 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0119 min-max -0.0095|0.0826\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0010 - sd : 0.0058 min-max -0.0323|0.0691\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1080 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0123 min-max -0.0070|0.0506\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0047 min-max -0.0468|0.0078\n",
      "Episode 1080 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.582 || 0.227 seconds, mem : 15657\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1085/2000  54% ETA:  0:01:00 |||||||||||||||||||||                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1090 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0136 min-max -0.0177|0.0849\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0008 - sd : 0.0050 min-max -0.0216|0.0675\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1090 \n",
      "update - q expected : mean : 0.0383 - sd : 0.0135 min-max -0.0104|0.0657\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0011 - sd : 0.0035 min-max -0.0251|0.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1098/2000  54% ETA:  0:00:59 |////////////////////                  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1100 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0137 min-max -0.0225|0.0853\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0051 min-max -0.0167|0.0654\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1100 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0137 min-max -0.0121|0.0514\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0039 min-max -0.0275|0.0082\n",
      "Episode 1100 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.577 || 0.231 seconds, mem : 15941\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1108/2000  55% ETA:  0:00:59 |---------------------                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1110 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0131 min-max -0.0235|0.0779\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0006 - sd : 0.0043 min-max -0.0323|0.0185\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1110 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0139 min-max -0.0179|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0012 - sd : 0.0036 min-max -0.0180|0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1118/2000  55% ETA:  0:00:58 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1120 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0145 min-max -0.0245|0.0685\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0042 min-max -0.0185|0.0229\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1120 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0158 min-max -0.0199|0.0695\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0013 - sd : 0.0048 min-max -0.0284|0.0727\n",
      "Episode 1120 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.571 || 0.241 seconds, mem : 16225\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1128/2000  56% ETA:  0:00:58 ||||||||||||||||||||||                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1130 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0145 min-max -0.0327|0.0771\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0011 - sd : 0.0043 min-max -0.0313|0.0240\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1130 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0155 min-max -0.0250|0.0673\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0016 - sd : 0.0052 min-max -0.0189|0.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1140/2000  57% ETA:  0:00:57 |/////////////////////                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1140 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0134 min-max -0.0280|0.0565\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0038 min-max -0.0195|0.0180\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1140 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0140 min-max -0.0164|0.0705\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0068 min-max -0.0201|0.0964\n",
      "Episode 1140 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.565 || 0.231 seconds, mem : 16509\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1151/2000  57% ETA:  0:00:57 |---------------------                 | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1150 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0130 min-max -0.0211|0.0512\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0044 min-max -0.0512|0.0153\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1150 \n",
      "update - q expected : mean : 0.0381 - sd : 0.0132 min-max -0.0132|0.0535\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0045 min-max -0.0100|0.0900\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0040 min-max -0.0179|0.0365\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1160 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0130 min-max -0.0181|0.0525\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0009 - sd : 0.0039 min-max -0.0233|0.0173\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1160 \n",
      "update - q expected : mean : 0.0380 - sd : 0.0134 min-max -0.0118|0.0526\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0012 - sd : 0.0038 min-max -0.0177|0.0080\n",
      "Episode 1160 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.560 || 0.228 seconds, mem : 16793\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1163/2000  58% ETA:  0:00:56 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1170 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0131 min-max -0.0203|0.0750\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0052 min-max -0.0294|0.0856\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1170 \n",
      "update - q expected : mean : 0.0385 - sd : 0.0132 min-max -0.0128|0.0702\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0011 - sd : 0.0042 min-max -0.0326|0.0101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1174/2000  58% ETA:  0:00:56 |||||||||||||||||||||||                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1180 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0130 min-max -0.0199|0.0727\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0038 min-max -0.0258|0.0148\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1180 \n",
      "update - q expected : mean : 0.0382 - sd : 0.0136 min-max -0.0182|0.0560\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0036 min-max -0.0166|0.0146\n",
      "Episode 1180 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.554 || 0.213 seconds, mem : 17077\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1187/2000  59% ETA:  0:00:55 |//////////////////////                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1190 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0131 min-max -0.0189|0.0717\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0036 min-max -0.0344|0.0134\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1190 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0145 min-max -0.0245|0.0513\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0038 min-max -0.0375|0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1200/2000  60% ETA:  0:00:54 |----------------------                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1200 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0139 min-max -0.0197|0.0795\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0040 min-max -0.0288|0.0226\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1200 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0142 min-max -0.0201|0.0587\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0035 min-max -0.0207|0.0156\n",
      "Episode 1200 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.548 || 0.230 seconds, mem : 17361\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1210/2000  60% ETA:  0:00:54 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\                | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1210 \n",
      "update - q expected : mean : 0.0378 - sd : 0.0123 min-max -0.0180|0.0528\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0038 min-max -0.0177|0.0190\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1210 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0135 min-max -0.0193|0.0594\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0001 - sd : 0.0055 min-max -0.0202|0.0944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1217/2000  60% ETA:  0:00:54 ||||||||||||||||||||||||               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1220 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0130 min-max -0.0210|0.0658\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0036 min-max -0.0183|0.0133\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1220 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0136 min-max -0.0205|0.0516\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0056 min-max -0.0137|0.1033\n",
      "Episode 1220 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.543 || 0.299 seconds, mem : 17645\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1227/2000  61% ETA:  0:00:53 |///////////////////////               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1230 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0130 min-max -0.0187|0.0531\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0033 min-max -0.0142|0.0160\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1230 \n",
      "update - q expected : mean : 0.0379 - sd : 0.0129 min-max -0.0158|0.0510\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0034 min-max -0.0132|0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1239/2000  61% ETA:  0:00:52 |-----------------------               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1240 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0135 min-max -0.0145|0.0507\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0036 min-max -0.0165|0.0164\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1240 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0140 min-max -0.0162|0.0518\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0031 min-max -0.0238|0.0081\n",
      "Episode 1240 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.538 || 0.201 seconds, mem : 17929\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1251/2000  62% ETA:  0:00:52 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\               | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1250 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0134 min-max -0.0199|0.0514\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0031 min-max -0.0149|0.0152\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1250 \n",
      "update - q expected : mean : 0.0380 - sd : 0.0131 min-max -0.0143|0.0556\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0032 min-max -0.0172|0.0103\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1260 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0124 min-max -0.0174|0.0508\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0037 min-max -0.0173|0.0113\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1260 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0149 min-max -0.0164|0.0749\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0055 min-max -0.0292|0.0922\n",
      "Episode 1260 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.532 || 0.164 seconds, mem : 18213\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1266/2000  63% ETA:  0:00:51 |||||||||||||||||||||||||              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1270 \n",
      "update - q expected : mean : 0.0374 - sd : 0.0130 min-max -0.0243|0.0506\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0058 min-max -0.0595|0.0856\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1270 \n",
      "update - q expected : mean : 0.0377 - sd : 0.0133 min-max -0.0102|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0038 min-max -0.0467|0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1281/2000  64% ETA:  0:00:50 |////////////////////////              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1280 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0148 min-max -0.0225|0.0505\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0053 min-max -0.0164|0.0998\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1280 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0149 min-max -0.0182|0.0503\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0030 min-max -0.0147|0.0113\n",
      "Episode 1280 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.527 || 0.162 seconds, mem : 18497\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1290 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0139 min-max -0.0198|0.0515\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0034 min-max -0.0170|0.0123\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1290 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0151 min-max -0.0192|0.0502\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0031 min-max -0.0151|0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1296/2000  64% ETA:  0:00:49 |------------------------              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1300 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0149 min-max -0.0200|0.0642\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0004 - sd : 0.0073 min-max -0.0138|0.1080\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1300 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0151 min-max -0.0190|0.0511\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0033 min-max -0.0127|0.0111\n",
      "Episode 1300 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.522 || 0.168 seconds, mem : 18781\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1311/2000  65% ETA:  0:00:48 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\              | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1310 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0149 min-max -0.0177|0.0519\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0033 min-max -0.0134|0.0133\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1310 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0148 min-max -0.0190|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0032 min-max -0.0156|0.0126\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1320 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0149 min-max -0.0264|0.0513\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0035 min-max -0.0310|0.0164\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1320 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0149 min-max -0.0229|0.0636\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0040 min-max -0.0440|0.0129\n",
      "Episode 1320 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.517 || 0.159 seconds, mem : 19065\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1327/2000  66% ETA:  0:00:46 ||||||||||||||||||||||||||             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1330 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0135 min-max -0.0184|0.0531\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0034 min-max -0.0187|0.0144\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1330 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0140 min-max -0.0206|0.0512\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0035 min-max -0.0243|0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1341/2000  67% ETA:  0:00:46 |/////////////////////////             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1340 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0132 min-max -0.0162|0.0503\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0043 min-max -0.0475|0.0113\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1340 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0133 min-max -0.0140|0.0515\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0043 min-max -0.0100|0.0900\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0044 min-max -0.0356|0.0549\n",
      "Episode 1340 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.511 || 0.189 seconds, mem : 19349\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1350 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0132 min-max -0.0185|0.0507\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0029 min-max -0.0119|0.0131\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1350 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0131 min-max -0.0136|0.0523\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0053 min-max -0.0175|0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1355/2000  67% ETA:  0:00:45 |-------------------------             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1360 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0136 min-max -0.0234|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0030 min-max -0.0130|0.0173\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1360 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0133 min-max -0.0188|0.0518\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0031 min-max -0.0169|0.0161\n",
      "Episode 1360 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.506 || 0.245 seconds, mem : 19633\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1367/2000  68% ETA:  0:00:44 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\             | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1370 \n",
      "update - q expected : mean : 0.0373 - sd : 0.0137 min-max -0.0165|0.0503\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0003 - sd : 0.0055 min-max -0.0155|0.1021\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1370 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0140 min-max -0.0140|0.0513\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0029 min-max -0.0142|0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1380/2000  69% ETA:  0:00:43 |||||||||||||||||||||||||||            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1380 \n",
      "update - q expected : mean : 0.0376 - sd : 0.0131 min-max -0.0178|0.0510\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0031 min-max -0.0164|0.0084\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1380 \n",
      "update - q expected : mean : 0.0375 - sd : 0.0137 min-max -0.0131|0.0622\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0031 min-max -0.0164|0.0106\n",
      "Episode 1380 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.501 || 0.222 seconds, mem : 19917\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1391/2000  69% ETA:  0:00:42 |//////////////////////////            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1390 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0143 min-max -0.0170|0.0508\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0055 min-max -0.0167|0.1014\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1390 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0143 min-max -0.0158|0.0513\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0031 min-max -0.0172|0.0110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1402/2000  70% ETA:  0:00:42 |--------------------------            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1400 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0147 min-max -0.0221|0.0514\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0052 min-max -0.0125|0.0982\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1400 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0148 min-max -0.0180|0.0523\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0029 min-max -0.0174|0.0105\n",
      "Episode 1400 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.496 || 0.238 seconds, mem : 20201\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1410 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0165 min-max -0.0248|0.0509\n",
      "\u001b[42mupdate - reward : mean : -0.0004 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0056 min-max -0.0140|0.1041\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1410 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0150 min-max -0.0199|0.0498\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0028 min-max -0.0160|0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1414/2000  70% ETA:  0:00:41 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\            | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1420 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0146 min-max -0.0221|0.0508\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0032 min-max -0.0172|0.0121\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1420 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0153 min-max -0.0196|0.0501\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0029 min-max -0.0160|0.0114\n",
      "Episode 1420 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.491 || 0.263 seconds, mem : 20485\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1427/2000  71% ETA:  0:00:40 ||||||||||||||||||||||||||||           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1430 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0150 min-max -0.0207|0.0512\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0033 min-max -0.0161|0.0107\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1430 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0149 min-max -0.0188|0.0512\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0030 min-max -0.0137|0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1438/2000  71% ETA:  0:00:39 |///////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1440 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0135 min-max -0.0163|0.0523\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0033 min-max -0.0178|0.0117\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1440 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0142 min-max -0.0155|0.0515\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0033 min-max -0.0306|0.0121\n",
      "Episode 1440 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.486 || 0.298 seconds, mem : 20769\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1446/2000  72% ETA:  0:00:39 |---------------------------           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1450 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0138 min-max -0.0181|0.0505\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0032 min-max -0.0150|0.0094\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1450 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0147 min-max -0.0165|0.0513\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0053 min-max -0.0152|0.0946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1458/2000  72% ETA:  0:00:38 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1460 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0141 min-max -0.0174|0.0501\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0032 min-max -0.0117|0.0133\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1460 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0154 min-max -0.0180|0.0519\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0027 min-max -0.0170|0.0095\n",
      "Episode 1460 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.482 || 0.341 seconds, mem : 21053\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1465/2000  73% ETA:  0:00:38 ||||||||||||||||||||||||||||           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1470 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0135 min-max -0.0197|0.0495\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0030 min-max -0.0123|0.0130\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1470 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0139 min-max -0.0134|0.0501\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0028 min-max -0.0162|0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1473/2000  73% ETA:  0:00:38 |///////////////////////////           | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1480 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0150 min-max -0.0201|0.0518\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0053 min-max -0.0207|0.0908\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1480 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0152 min-max -0.0158|0.0569\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0002 - sd : 0.0064 min-max -0.0368|0.0889\n",
      "Episode 1480 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.477 || 0.309 seconds, mem : 21337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1482/2000  74% ETA:  0:00:37 |----------------------------          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1490 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0151 min-max -0.0182|0.0515\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0030 min-max -0.0159|0.0115\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1490 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0150 min-max -0.0134|0.0526\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0030 min-max -0.0193|0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1493/2000  74% ETA:  0:00:36 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1500 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0139 min-max -0.0176|0.0710\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0028 min-max -0.0189|0.0082\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1500 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0143 min-max -0.0171|0.0506\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0026 min-max -0.0145|0.0093\n",
      "Episode 1500 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.472 || 0.266 seconds, mem : 21621\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1503/2000  75% ETA:  0:00:36 |||||||||||||||||||||||||||||          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1510 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0141 min-max -0.0138|0.0501\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0030 min-max -0.0181|0.0078\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1510 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0155 min-max -0.0172|0.0511\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0026 min-max -0.0115|0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1513/2000  75% ETA:  0:00:35 |////////////////////////////          | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1520 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0137 min-max -0.0152|0.0501\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0008 - sd : 0.0029 min-max -0.0124|0.0066\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1520 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0158 min-max -0.0201|0.0617\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0006 - sd : 0.0045 min-max -0.0112|0.0805\n",
      "Episode 1520 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.467 || 0.331 seconds, mem : 21905\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1531/2000  76% ETA:  0:00:34 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1530 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0151 min-max -0.0166|0.0544\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0052 min-max -0.0123|0.0960\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1530 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0156 min-max -0.0179|0.0500\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0032 min-max -0.0192|0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1540/2000  77% ETA:  0:00:34 ||||||||||||||||||||||||||||||         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1540 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0151 min-max -0.0210|0.0508\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0030 min-max -0.0184|0.0110\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1540 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0150 min-max -0.0200|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0026 min-max -0.0159|0.0100\n",
      "Episode 1540 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.463 || 0.328 seconds, mem : 22189\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1548/2000  77% ETA:  0:00:33 |/////////////////////////////         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1550 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0148 min-max -0.0216|0.0853\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0044 min-max -0.0140|0.0733\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1550 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0135 min-max -0.0154|0.0585\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0032 min-max -0.0209|0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1558/2000  77% ETA:  0:00:32 |-----------------------------         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1560 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0142 min-max -0.0234|0.0711\n",
      "\u001b[42mupdate - reward : mean : 0.0001 - sd : 0.0046 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0018 - sd : 0.0052 min-max -0.0227|0.0857\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1560 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0136 min-max -0.0221|0.0501\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0013 - sd : 0.0032 min-max -0.0138|0.0121\n",
      "Episode 1560 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.458 || 0.254 seconds, mem : 22473\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1569/2000  78% ETA:  0:00:32 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\         | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1570 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0154 min-max -0.0179|0.0724\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0009 - sd : 0.0032 min-max -0.0240|0.0096\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1570 \n",
      "update - q expected : mean : 0.0366 - sd : 0.0148 min-max -0.0143|0.0754\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0005 - sd : 0.0041 min-max -0.0146|0.0678\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1580 \n",
      "update - q expected : mean : 0.0370 - sd : 0.0139 min-max -0.0160|0.0685\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0030 min-max -0.0236|0.0095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1581/2000  79% ETA:  0:00:31 |||||||||||||||||||||||||||||||        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 1 and episode 1580 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0143 min-max -0.0120|0.0521\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0029 min-max -0.0139|0.0104\n",
      "Episode 1580 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.454 || 0.492 seconds, mem : 22757\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1592/2000  79% ETA:  0:00:30 |//////////////////////////////        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1590 \n",
      "update - q expected : mean : 0.0372 - sd : 0.0130 min-max -0.0127|0.0520\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0015 - sd : 0.0033 min-max -0.0191|0.0101\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1590 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0137 min-max -0.0097|0.0504\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0039 min-max -0.0514|0.0091\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1600 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0138 min-max -0.0139|0.0525\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0029 min-max -0.0187|0.0116\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1600 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0144 min-max -0.0149|0.0830\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0000 - sd : 0.0039 min-max -0.0171|0.0605\n",
      "Episode 1600 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.449 || 0.233 seconds, mem : 23041\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1604/2000  80% ETA:  0:00:29 |------------------------------        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1610 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0147 min-max -0.0208|0.0498\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0029 min-max -0.0167|0.0114\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1610 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0147 min-max -0.0158|0.0512\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0027 min-max -0.0115|0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1615/2000  80% ETA:  0:00:29 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1620 \n",
      "update - q expected : mean : 0.0350 - sd : 0.0159 min-max -0.0221|0.0515\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0006 - sd : 0.0030 min-max -0.0192|0.0121\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1620 \n",
      "update - q expected : mean : 0.0354 - sd : 0.0162 min-max -0.0209|0.0516\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0031 min-max -0.0163|0.0109\n",
      "Episode 1620 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.445 || 0.209 seconds, mem : 23325\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1627/2000  81% ETA:  0:00:28 |||||||||||||||||||||||||||||||        | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1630 \n",
      "update - q expected : mean : 0.0352 - sd : 0.0157 min-max -0.0201|0.0807\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0013 - sd : 0.0045 min-max -0.0329|0.0661\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1630 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0152 min-max -0.0226|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0005 - sd : 0.0027 min-max -0.0138|0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1638/2000  81% ETA:  0:00:27 |///////////////////////////////       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1640 \n",
      "update - q expected : mean : 0.0349 - sd : 0.0159 min-max -0.0216|0.0763\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0007 - sd : 0.0046 min-max -0.0185|0.0758\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1640 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0158 min-max -0.0209|0.0513\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0002 - sd : 0.0029 min-max -0.0136|0.0109\n",
      "Episode 1640 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.440 || 0.223 seconds, mem : 23609\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1649/2000  82% ETA:  0:00:26 |-------------------------------       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1650 \n",
      "update - q expected : mean : 0.0365 - sd : 0.0140 min-max -0.0136|0.0837\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0037 min-max -0.0126|0.0624\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1650 \n",
      "update - q expected : mean : 0.0369 - sd : 0.0139 min-max -0.0162|0.0509\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0025 min-max -0.0122|0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1659/2000  82% ETA:  0:00:25 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1660 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0131 min-max -0.0109|0.0507\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0028 min-max -0.0165|0.0066\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1660 \n",
      "update - q expected : mean : 0.0368 - sd : 0.0140 min-max -0.0167|0.0519\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0031 min-max -0.0246|0.0138\n",
      "Episode 1660 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.436 || 0.214 seconds, mem : 23893\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1670/2000  83% ETA:  0:00:25 ||||||||||||||||||||||||||||||||       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1670 \n",
      "update - q expected : mean : 0.0361 - sd : 0.0145 min-max -0.0132|0.0494\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0010 - sd : 0.0029 min-max -0.0152|0.0051\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1670 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0153 min-max -0.0158|0.0519\n",
      "update - reward : mean : -0.0005 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0028 min-max -0.0151|0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1681/2000  84% ETA:  0:00:24 |///////////////////////////////       | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1680 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0145 min-max -0.0141|0.0867\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0031 min-max -0.0346|0.0093\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1680 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0150 min-max -0.0191|0.0893\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0031 min-max -0.0410|0.0091\n",
      "Episode 1680 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.431 || 0.201 seconds, mem : 24177\n",
      "\u001b[0m--------------------------------------\n",
      "Agent 0 and episode 1690 \n",
      "update - q expected : mean : 0.0355 - sd : 0.0155 min-max -0.0232|0.0573\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0010 - sd : 0.0036 min-max -0.0287|0.0132\n",
      "--------------------------------------"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1691/2000  84% ETA:  0:00:23 |--------------------------------      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent 1 and episode 1690 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0144 min-max -0.0165|0.0506\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0032 min-max -0.0402|0.0126\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1700 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0144 min-max -0.0215|0.0564\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0030 min-max -0.0314|0.0134\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1700 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0148 min-max -0.0188|0.0514\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0032 min-max -0.0357|0.0098\n",
      "Episode 1700 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.427 || 0.196 seconds, mem : 24461\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1704/2000  85% ETA:  0:00:22 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1710 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0142 min-max -0.0094|0.0874\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0016 - sd : 0.0041 min-max -0.0469|0.0088\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1710 \n",
      "update - q expected : mean : 0.0355 - sd : 0.0156 min-max -0.0151|0.0622\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0009 - sd : 0.0032 min-max -0.0240|0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1716/2000  85% ETA:  0:00:21 |||||||||||||||||||||||||||||||||      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1720 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0150 min-max -0.0193|0.0590\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0051 min-max -0.0143|0.0998\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1720 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0150 min-max -0.0178|0.0504\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0028 min-max -0.0198|0.0087\n",
      "Episode 1720 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.423 || 0.226 seconds, mem : 24745\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1728/2000  86% ETA:  0:00:20 |////////////////////////////////      | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1730 \n",
      "update - q expected : mean : 0.0354 - sd : 0.0152 min-max -0.0190|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0024 min-max -0.0122|0.0090\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1730 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0158 min-max -0.0205|0.0617\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0030 min-max -0.0237|0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1739/2000  86% ETA:  0:00:19 |---------------------------------     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1740 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0145 min-max -0.0175|0.0503\n",
      "update - reward : mean : -0.0002 - sd : 0.0014 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0026 min-max -0.0172|0.0093\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1740 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0148 min-max -0.0178|0.0496\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0027 min-max -0.0197|0.0087\n",
      "Episode 1740 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.419 || 0.229 seconds, mem : 25029\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1751/2000  87% ETA:  0:00:19 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1750 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0140 min-max -0.0151|0.0586\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0027 min-max -0.0229|0.0090\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1750 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0138 min-max -0.0150|0.0644\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0049 min-max -0.0285|0.0909\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1760 \n",
      "update - q expected : mean : 0.0363 - sd : 0.0143 min-max -0.0145|0.0853\n",
      "\u001b[42mupdate - reward : mean : -0.0002 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0043 min-max -0.0172|0.0694\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1760 \n",
      "update - q expected : mean : 0.0371 - sd : 0.0134 min-max -0.0108|0.0577\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0012 - sd : 0.0052 min-max -0.0201|0.0926\n",
      "Episode 1760 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.414 || 0.206 seconds, mem : 25313\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1763/2000  88% ETA:  0:00:18 ||||||||||||||||||||||||||||||||||     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1770 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0139 min-max -0.0142|0.0773\n",
      "update - reward : mean : -0.0002 - sd : 0.0015 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0029 min-max -0.0247|0.0075\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1770 \n",
      "update - q expected : mean : 0.0367 - sd : 0.0135 min-max -0.0125|0.0501\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0030 min-max -0.0289|0.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1776/2000  88% ETA:  0:00:17 |/////////////////////////////////     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1780 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0149 min-max -0.0161|0.0516\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0024 min-max -0.0123|0.0070\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1780 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0149 min-max -0.0184|0.0511\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0029 min-max -0.0201|0.0085\n",
      "Episode 1780 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.410 || 0.227 seconds, mem : 25597\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1788/2000  89% ETA:  0:00:16 |---------------------------------     | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1790 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0148 min-max -0.0195|0.0699\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0047 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0047 min-max -0.0197|0.0861\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1790 \n",
      "update - q expected : mean : 0.0355 - sd : 0.0155 min-max -0.0190|0.0660\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0029 min-max -0.0161|0.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1799/2000  89% ETA:  0:00:15 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1800 \n",
      "update - q expected : mean : 0.0354 - sd : 0.0157 min-max -0.0197|0.0505\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0030 min-max -0.0196|0.0121\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1800 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0158 min-max -0.0174|0.0634\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0050 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0003 - sd : 0.0048 min-max -0.0154|0.0914\n",
      "Episode 1800 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.406 || 0.303 seconds, mem : 25881\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1808/2000  90% ETA:  0:00:14 |||||||||||||||||||||||||||||||||||    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1810 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0150 min-max -0.0205|0.0527\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0001 - sd : 0.0027 min-max -0.0112|0.0105\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1810 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0165 min-max -0.0244|0.0518\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0006 - sd : 0.0032 min-max -0.0174|0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1819/2000  90% ETA:  0:00:13 |//////////////////////////////////    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1820 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0144 min-max -0.0113|0.0504\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0025 min-max -0.0141|0.0065\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1820 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0147 min-max -0.0149|0.0494\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0029 min-max -0.0215|0.0112\n",
      "Episode 1820 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.402 || 0.222 seconds, mem : 26165\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1830/2000  91% ETA:  0:00:13 |----------------------------------    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1830 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0160 min-max -0.0193|0.0969\n",
      "\u001b[42mupdate - reward : mean : -0.0000 - sd : 0.0066 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0004 - sd : 0.0058 min-max -0.0183|0.0965\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1830 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0154 min-max -0.0136|0.0503\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0027 min-max -0.0161|0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1839/2000  91% ETA:  0:00:12 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\    | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1840 \n",
      "update - q expected : mean : 0.0358 - sd : 0.0146 min-max -0.0167|0.0504\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0025 min-max -0.0134|0.0100\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1840 \n",
      "update - q expected : mean : 0.0362 - sd : 0.0148 min-max -0.0156|0.0509\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0049 min-max -0.0146|0.0935\n",
      "Episode 1840 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.398 || 0.280 seconds, mem : 26449\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1848/2000  92% ETA:  0:00:11 ||||||||||||||||||||||||||||||||||||   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1850 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0153 min-max -0.0133|0.0495\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0025 min-max -0.0138|0.0091\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1850 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0152 min-max -0.0120|0.0552\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0011 - sd : 0.0030 min-max -0.0190|0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1855/2000  92% ETA:  0:00:11 |///////////////////////////////////   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1860 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0147 min-max -0.0168|0.0497\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0007 - sd : 0.0027 min-max -0.0167|0.0091\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1860 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0144 min-max -0.0155|0.0501\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0026 min-max -0.0157|0.0143\n",
      "Episode 1860 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.394 || 0.251 seconds, mem : 26733\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1866/2000  93% ETA:  0:00:10 |-----------------------------------   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1870 \n",
      "update - q expected : mean : 0.0347 - sd : 0.0153 min-max -0.0151|0.0491\n",
      "update - reward : mean : -0.0003 - sd : 0.0016 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0026 min-max -0.0138|0.0101\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1870 \n",
      "update - q expected : mean : 0.0352 - sd : 0.0154 min-max -0.0138|0.0506\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0000 - sd : 0.0025 min-max -0.0137|0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1878/2000  93% ETA:  0:00:09 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1880 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0143 min-max -0.0143|0.0697\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0000 - sd : 0.0027 min-max -0.0270|0.0099\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1880 \n",
      "update - q expected : mean : 0.0346 - sd : 0.0161 min-max -0.0188|0.0506\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0009 - sd : 0.0028 min-max -0.0161|0.0103\n",
      "Episode 1880 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.390 || 0.220 seconds, mem : 27017\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1889/2000  94% ETA:  0:00:08 ||||||||||||||||||||||||||||||||||||   | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1890 \n",
      "update - q expected : mean : 0.0350 - sd : 0.0153 min-max -0.0123|0.0493\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0026 min-max -0.0163|0.0100\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1890 \n",
      "update - q expected : mean : 0.0347 - sd : 0.0166 min-max -0.0149|0.0700\n",
      "\u001b[42mupdate - reward : mean : -0.0003 - sd : 0.0049 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0005 - sd : 0.0045 min-max -0.0134|0.0866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1901/2000  95% ETA:  0:00:07 |////////////////////////////////////  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1900 \n",
      "update - q expected : mean : 0.0351 - sd : 0.0156 min-max -0.0176|0.0904\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0004 - sd : 0.0029 min-max -0.0325|0.0101\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1900 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0161 min-max -0.0214|0.0519\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0002 - sd : 0.0029 min-max -0.0157|0.0114\n",
      "Episode 1900 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.386 || 0.229 seconds, mem : 27301\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1912/2000  95% ETA:  0:00:06 |------------------------------------  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1910 \n",
      "update - q expected : mean : 0.0350 - sd : 0.0155 min-max -0.0160|0.0551\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0024 min-max -0.0141|0.0078\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1910 \n",
      "update - q expected : mean : 0.0359 - sd : 0.0155 min-max -0.0160|0.0516\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0025 min-max -0.0162|0.0069\n",
      "--------------------------------------\n",
      "Agent 0 and episode 1920 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0154 min-max -0.0161|0.0502\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0026 min-max -0.0144|0.0179\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1920 \n",
      "update - q expected : mean : 0.0364 - sd : 0.0144 min-max -0.0105|0.0513\n",
      "update - reward : mean : -0.0004 - sd : 0.0021 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0013 - sd : 0.0033 min-max -0.0357|0.0082\n",
      "Episode 1920 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.383 || 0.245 seconds, mem : 27585\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1922/2000  96% ETA:  0:00:06 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1930 \n",
      "update - q expected : mean : 0.0355 - sd : 0.0148 min-max -0.0121|0.0575\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0004 - sd : 0.0025 min-max -0.0133|0.0097\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1930 \n",
      "update - q expected : mean : 0.0360 - sd : 0.0149 min-max -0.0149|0.0504\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0007 - sd : 0.0025 min-max -0.0148|0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1935/2000  96% ETA:  0:00:05 |||||||||||||||||||||||||||||||||||||  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1940 \n",
      "update - q expected : mean : 0.0348 - sd : 0.0158 min-max -0.0161|0.0499\n",
      "update - reward : mean : -0.0003 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0027 min-max -0.0285|0.0106\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1940 \n",
      "update - q expected : mean : 0.0355 - sd : 0.0155 min-max -0.0166|0.0650\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0003 - sd : 0.0029 min-max -0.0392|0.0093\n",
      "Episode 1940 || Reward : [ 0.   -0.01] || avg reward :  0.000 || Noise  0.379 || 0.203 seconds, mem : 27869\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1946/2000  97% ETA:  0:00:04 |////////////////////////////////////  | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1950 \n",
      "update - q expected : mean : 0.0350 - sd : 0.0163 min-max -0.0210|0.0938\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0009 - sd : 0.0037 min-max -0.0213|0.0596\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1950 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0145 min-max -0.0164|0.0502\n",
      "update - reward : mean : -0.0002 - sd : 0.0013 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0008 - sd : 0.0027 min-max -0.0129|0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1957/2000  97% ETA:  0:00:03 |------------------------------------- | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1960 \n",
      "update - q expected : mean : 0.0354 - sd : 0.0156 min-max -0.0179|0.0529\n",
      "\u001b[42mupdate - reward : mean : 0.0000 - sd : 0.0065 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : -0.0000 - sd : 0.0066 min-max -0.0128|0.0994\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1960 \n",
      "update - q expected : mean : 0.0349 - sd : 0.0160 min-max -0.0171|0.0514\n",
      "update - reward : mean : -0.0004 - sd : 0.0018 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0003 - sd : 0.0025 min-max -0.0129|0.0099\n",
      "Episode 1960 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.375 || 0.260 seconds, mem : 28153\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1968/2000  98% ETA:  0:00:02 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1970 \n",
      "update - q expected : mean : 0.0357 - sd : 0.0146 min-max -0.0137|0.0497\n",
      "update - reward : mean : -0.0004 - sd : 0.0019 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0006 - sd : 0.0023 min-max -0.0154|0.0075\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1970 \n",
      "update - q expected : mean : 0.0350 - sd : 0.0160 min-max -0.0202|0.0788\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0048 min-max -0.0100|0.1000\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0008 - sd : 0.0043 min-max -0.0136|0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1980/2000  99% ETA:  0:00:01 |||||||||||||||||||||||||||||||||||||| | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1980 \n",
      "update - q expected : mean : 0.0353 - sd : 0.0154 min-max -0.0114|0.0501\n",
      "update - reward : mean : -0.0005 - sd : 0.0022 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0013 - sd : 0.0027 min-max -0.0130|0.0054\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1980 \n",
      "update - q expected : mean : 0.0352 - sd : 0.0158 min-max -0.0154|0.0495\n",
      "update - reward : mean : -0.0004 - sd : 0.0020 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : -0.0005 - sd : 0.0025 min-max -0.0148|0.0093\n",
      "Episode 1980 || Reward : [-0.01  0.  ] || avg reward :  0.000 || Noise  0.371 || 0.245 seconds, mem : 28437\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 1991/2000  99% ETA:  0:00:00 |///////////////////////////////////// | \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Agent 0 and episode 1990 \n",
      "update - q expected : mean : 0.0347 - sd : 0.0152 min-max -0.0163|0.0496\n",
      "update - reward : mean : -0.0003 - sd : 0.0017 min-max -0.0100|0.0000\n",
      "update - TD-Error : mean : 0.0001 - sd : 0.0032 min-max -0.0447|0.0095\n",
      "--------------------------------------\n",
      "Agent 1 and episode 1990 \n",
      "update - q expected : mean : 0.0356 - sd : 0.0149 min-max -0.0171|0.0511\n",
      "\u001b[42mupdate - reward : mean : -0.0001 - sd : 0.0044 min-max -0.0100|0.0900\n",
      "\u001b[0mupdate - TD-Error : mean : 0.0001 - sd : 0.0032 min-max -0.0141|0.0480\n",
      "Config Parameters    : \n",
      "gamma                : 0.99\n",
      "tau                  : 0.01\n",
      "action_size          : 2\n",
      "state_size           : 24\n",
      "hidden_size          : 256\n",
      "buffer_size          : 50000\n",
      "batch_size           : 512\n",
      "dropout              : 0.01\n",
      "seed                 : 89\n",
      "max_episodes         : 2000\n",
      "learn_every          : 10\n",
      "critic_learning_rate : 0.001\n",
      "actor_learning_rate  : 0.001\n",
      "noise_decay          : 0.9995\n",
      "num_agents           : 2\n",
      "env_file_name        : Tennis_Windows_x86_64/Tennis.exe\n",
      "train_mode           : True\n",
      "brain_name           : TennisBrain\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from maddpg import maddpg\n",
    "import cProfile\n",
    "DoProfile = False\n",
    "\n",
    "config = {\n",
    "    'gamma'               : 0.99,\n",
    "    'tau'                 : 0.01,\n",
    "    'action_size'         : action_size,\n",
    "    'state_size'          : state_size,\n",
    "    'hidden_size'         : 256,\n",
    "    'buffer_size'         : 50000,\n",
    "    'batch_size'          : 512,\n",
    "    'dropout'             : 0.01,\n",
    "    'seed'                : 98,\n",
    "    'max_episodes'        : 2000,\n",
    "    'learn_every'         : 10,\n",
    "    'critic_learning_rate': 1e-3,\n",
    "    'actor_learning_rate' : 1e-3,\n",
    "    'noise_decay'         : 0.9995,\n",
    "    'num_agents'          : num_agents,\n",
    "    'env_file_name'       : env_file_name,\n",
    "    'train_mode'          : True,\n",
    "    'brain_name'          : brain_name}\n",
    "\n",
    "def print_config(config):\n",
    "    print('Config Parameters    : ')\n",
    "    for c,k in config.items():\n",
    "        print('{:20s} : {}'.format(c,k))\n",
    "\n",
    "# seed_range = [0.1, 0.05, 0.03, 0.01, 0.005, 0.003, 0.001]\n",
    "for main in range(10):#len(tau_range)):\n",
    "    config['seed'] += 1\n",
    "    print_config(config)\n",
    "    agent = maddpg(env, config)\n",
    "    if DoProfile:cProfile.run(\"results = agent.train()\",'PerfStats')\n",
    "    else:results = agent.train()\n",
    "    # all_rewards,avg_rewards,critic_losses,actor_losses = agent.train()\n",
    "    print_config(config)\n",
    "    plot_results(results)\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
